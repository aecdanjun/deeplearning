{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction\n",
    "\n",
    "This chapter covers\n",
    "- Forms of machine learning beyond classification and regression\n",
    "- Formal evaluation procedures for machine learning models\n",
    "- Preparing data for deep learning\n",
    "- Feature engineering\n",
    "- Tackling overfitting\n",
    "- The universal workflow for approaching machine learning problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Four branches of machine learning\n",
    "\n",
    "- **Supervised learning**\n",
    "    - This is by far the most common case. It consists of learning to map input data to known targets (also called annotations), given a set of examples (often annotated by humans). **Supervised learning by far is the dominant form of deep learning today, with a wide range of industry applications.**\n",
    "- **Unsupervised learning**\n",
    "    - This branch of machine learning consists of finding interesting transformations of the input data without the help of any targets, for the purposes of data visualization, data compression, or data denoising, or to better understand the correlations present in the data at hand. Unsupervised learning is the bread and butter of data analytics, and it’s often a necessary step in better understanding a dataset before attempting to solve a supervised-learning problem. **Dimensionality reduction** and **clustering** are well-known categories of unsupervised learning.\n",
    "- **Self-supervised learning**\n",
    "    - It is a supervised learning without human-annotated labels (they’re generated from the input data, typically using a\n",
    "heuristic algorithm).\n",
    "- **Reinforcement learning**\n",
    "    - In reinforcement learning, an agent receives information about its environment and learns to choose actions that\n",
    "will maximize some reward. Currently, reinforcement learning is mostly a research area and hasn’t yet had significant\n",
    "practical successes beyond games. In time, however, we expect to see reinforcement learning take over an increasingly large range of real-world applications: self-driving cars, robotics, resource management, education, and so on. It’s an idea\n",
    "whose time has come, or will come soon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Evaluating machine-learning models\n",
    "\n",
    "In machine learning, the goal is to achieve models that **generalize** — that perform well on never-before-seen data—and **overfitting** is the central obstacle. In this section, we’ll focus on **how to measure generalization**: how to evaluate machine-learning models.\n",
    "\n",
    "## 3.1 Training, validation and test sets\n",
    "\n",
    "Evaluating a model always boils down to splitting the available data into three sets: \n",
    "\n",
    "- **training**\n",
    "- **validation**\n",
    "- **test**\n",
    "\n",
    "You train on the training data and evaluate your model on the validation data. Once your model is ready for prime time, you test it one final time on the test data.\n",
    "\n",
    "Splitting your data into training, validation, and test sets may seem straightforward, but there are a few advanced ways to do it that can come in handy when little data is available.\n",
    "\n",
    "- **Hold-out validation**\n",
    "- **K-Fold**\n",
    "- **Iterated K-Fold validation with shuffling**\n",
    "\n",
    "<img width=\"800\" alt=\"creating a repo\" src=\"https://drive.google.com/uc?export=view&id=1zP9sqXEYvfNLyII_avHuF9JYk2-gBF0O\">\n",
    "\n",
    ">```python\n",
    "# Hold-out validation pseudo-code\n",
    "num_validation_samples = 10000\n",
    "# Shuffling the data is usually appropriate.\n",
    "np.random.shuffle(data)\n",
    "# Defines the validation set\n",
    "validation_data = data[:num_validation_samples]\n",
    "data = data[num_validation_samples:]\n",
    "# Defines the training set\n",
    "training_data = data[:]\n",
    "# Trains a model on the training data, and evaluates it on the validation data\n",
    "model = get_model()\n",
    "model.train(training_data)\n",
    "validation_score = model.evaluate(validation_data)\n",
    "# At this point you can tune your model, retrain it, evaluate it, tune it again...\n",
    "# Once you’ve tuned your hyperparameters, it’s common to train your final model\n",
    "# from scratch on all non-test data available.\n",
    "model = get_model()\n",
    "model.train(np.concatenate([training_data,\n",
    "validation_data]))\n",
    "test_score = model.evaluate(test_data)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">```python\n",
    "# K-Fold Validation\n",
    "k = 4\n",
    "num_validation_samples = len(data) // k\n",
    "np.random.shuffle(data)\n",
    "validation_scores = []\n",
    "for fold in range(k):\n",
    "    # Selects the validation data partition\n",
    "    validation_data = data[num_validation_samples * fold:num_validation_samples * (fold + 1)]\n",
    "    # Uses the remainder of the data as training data. Note that the + operator is list concatenation, not summation\n",
    "    training_data = data[:num_validation_samples * fold] + data[num_validation_samples * (fold + 1):]\n",
    "    # Creates a brand-new instance of the model (untrained)\n",
    "    model = get_model()\n",
    "    model.train(training_data)\n",
    "    validation_score = model.evaluate(validation_data)\n",
    "    validation_scores.append(validation_score)\n",
    "# Validation score: average of the validation scores of the k folds\n",
    "validation_score = np.average(validation_scores)\n",
    "# Trains the final model on all nontest data available\n",
    "model = get_model()\n",
    "model.train(data)\n",
    "test_score = model.evaluate(test_data)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Data preprocessing and feature engineering\n",
    "\n",
    "In addition to model evaluation, an important question we must tackle before we dive deeper into model development is the following: how do you prepare the input data and targets before feeding them into a neural network?\n",
    "\n",
    "###  3.2.1 Data preprocessing\n",
    "\n",
    "Data preprocessing aims at making the raw data at hand more amenable to neural networks. This includes:\n",
    "- **vectorization**\n",
    "- **normalization**\n",
    "- **handling missing values**\n",
    "- **feature extraction**\n",
    "\n",
    "### 3.2.2 Feature engineering\n",
    "\n",
    "The essence of **feature engineering** is making a problem easier by **expressing it in a simpler way**. It usually requires understanding the problem in depth.\n",
    "\n",
    "<img width=\"400\" alt=\"creating a repo\" src=\"https://drive.google.com/uc?export=view&id=14dhxjfqcJlg4SCQHURYhLHPuDKQgoWfT\">\n",
    "\n",
    "Modern deep learning removes the need for most feature engineering, because neural networks are capable of automatically extracting useful features from raw data. Does this mean you don’t have to worry about feature engineering as\n",
    "long as you’re using deep neural networks? No, for two reasons:\n",
    "\n",
    "- **Good features** still allow you to solve problems more elegantly while **using fewer resources**. For instance, it would be ridiculous to solve the problem of reading a clock face using a convolutional neural network.\n",
    "- **Good features** let you solve a problem with far **less data**. The ability of deep learning models to learn features on their own relies on having lots of training data available; if you have only a few samples, then the information value in\n",
    "their features becomes critical."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Overfitting and underfitting\n",
    "\n",
    "At the **beginning of training**, optimization and generalization are correlated: **the lower the loss on training data, the lower the loss on test data**. While this is happening, your model is said to be **underfit**: there is still progress to be made; the network hasn’t yet modeled all relevant patterns in the training data. \n",
    "\n",
    "But after a certain number of iterations on the training data, **generalization stops improving, and validation metrics stall and then begin to degrade**: the model is starting to **overfit**. That is, it’s beginning to learn patterns that are specific to the training data but that are misleading or irrelevant when it comes to new data.\n",
    "\n",
    "To prevent a model from learning misleading or irrelevant patterns found in the training data, the best solution is to get more training data. \n",
    "- **A model trained on more data will naturally generalize better**. \n",
    "\n",
    "When that isn’t possible, the next-best solution:\n",
    "- to modulate the quantity of information that your model is allowed to store\n",
    "- to add constraints on what information it’s allowed to store. \n",
    "\n",
    "If a network can only afford to memorize a small number of patterns, the optimization process will force it to focus on the **most prominent patterns**, which have a better chance of generalizing well. The processing of fighting overfitting this way is called **regularization**.\n",
    "\n",
    "## 3.3.1 Regularization\n",
    "\n",
    "These are the most common ways to prevent overfitting in neural networks:\n",
    "- Get more training data.\n",
    "- Reduce the capacity of the network.\n",
    "- Add weight regularization.\n",
    "- Add dropout.\n",
    "\n",
    "### 3.3.1.1 Reducing the network’s size\n",
    "\n",
    "The general workflow to find an appropriate model size is:\n",
    "\n",
    "1. Start with relatively few layers and parameters\n",
    "2. Increase the size of the layers or add new layers until you see diminishing returns with regard to validation loss\n",
    "\n",
    "Let’s try this on the movie-review classification network:\n",
    "\n",
    "- Original model\n",
    ">```python\n",
    "from keras import models\n",
    "from keras import layers\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(16, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(16, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "```\n",
    "\n",
    "- Version of the model with lower capacity\n",
    ">```python\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(4, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(4, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "```\n",
    "\n",
    "- Version of the model with higher capacity\n",
    ">```python\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(512, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(512, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "```\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "    <td> <img src=\"https://drive.google.com/uc?export=view&id=1VMkzNR_R8ODITTtwCAmLZzjIZfJTN11x\" width=\"400\"> </td>\n",
    "    <td> <img src=\"https://drive.google.com/uc?export=view&id=1-C0mb-NPmlyCFv-HMkSbATOVywmC9LVZ\" width=\"400\"> </td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "As you can see, **the smaller network starts overfitting later** than the reference network (after six epochs rather than four), and its performance degrades more slowly once it starts overfitting.\n",
    "\n",
    "**The bigger network starts overfitting almost immediately**, after just one epoch, and it overfits much more severely. Its validation loss is also noisier.\n",
    "\n",
    "\n",
    "### 3.3.1.2 Adding weight regularization\n",
    "\n",
    "**A common way to mitigate overfitting** is to put constraints on the complexity of a network by **forcing its weights to take only small values**, which makes the distribution of weight values more regular. This is called **weight regularization**, and it’s done by adding to the loss function of the network a cost associated with having large weights.\n",
    "\n",
    "- **L1 regularization**\n",
    "    - The cost added is proportional to the absolute value of the weight coefficients (the L1 norm of the weights).\n",
    "- **L2 regularization**\n",
    "    - The cost added is proportional to the square of the value of the weight coefficients (the L2 norm of the weights)\n",
    "    \n",
    ">```python\n",
    "# Adding L2 weight regularization to the model\n",
    "from keras import regularizers\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(16, kernel_regularizer=regularizers.l2(0.001),\n",
    "activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(16, kernel_regularizer=regularizers.l2(0.001),\n",
    "activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "```\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "    <td> <img src=\"https://drive.google.com/uc?export=view&id=1WwK9JJhPyf_WcYlB1LabnRvWuTLjVQNy\" width=\"400\"> </td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "As you can see, the model with L2 regularization (dots) has become much more resistant to overfitting than the reference model (crosses), even though both models have the same number of parameters.\n",
    "\n",
    "### 3.3.1.3 Adding dropout\n",
    "\n",
    "**Dropout** is one of the most effective and most commonly used regularization techniques for neural networks, developed by [Geoff Hinton](https://en.wikipedia.org/wiki/Geoffrey_Hinton) and his students at the University of Toronto. Dropout, applied to a layer, consists of randomly dropping out (setting to zero) a number of output features of the layer during training.\n",
    "\n",
    "<img width=\"400\" alt=\"creating a repo\" src=\"https://drive.google.com/uc?export=view&id=1RXqlzlb7oN7Q3e5ojctwmEXBUCk1ode2\">\n",
    "\n",
    "This technique may seem strange and arbitrary. Why would this help reduce overfitting? Hinton says he was inspired by, among other things, a fraud-prevention mechanism used by banks. In his own words, “I went to my bank. The tellers kept changing\n",
    "and I asked one of them why. He said he didn’t know but they got moved around a lot. I figured it must be because it would require cooperation between employees to successfully defraud the bank. This made me realize that randomly **removing a different subset of neurons on each example would prevent conspiracies and thus reduce overfitting**.”\n",
    "\n",
    "- The core idea is that introducing noise in the output values of a layer can break up happenstance patterns that aren’t significant (what Hinton refers to as conspiracies), which the network will start memorizing if no noise is present.\n",
    "\n",
    ">```python\n",
    "# Adding dropout to the IMDB network\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(16, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(16, activation='relu'))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "```\n",
    "\n",
    "<img width=\"400\" alt=\"creating a repo\" src=\"https://drive.google.com/uc?export=view&id=1PjdL0qXfg2AqwiY5Yp7H_lx8pnw-ETZD\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. The universal workflow of machine learning\n",
    "\n",
    "1. **Defining the problem and assembling a dataset**\n",
    "    - you must define the problem at hand\n",
    "    - machine learning can only be used to memorize patterns that are present in your training data. You can only recognize what you’ve seen before.\n",
    "2. **Choosing a measure of success**\n",
    "    - your metric for success will guide the choice of a loss function: what your model will optimize.\n",
    "3. **Deciding on an evaluation protocol**\n",
    "    - hold-out validation set\n",
    "    - K-fold cross-validation\n",
    "    - iterated K-fold validation\n",
    "    - note: in most cases, the first will work well enough\n",
    "4. **Preparing your data**\n",
    "    - your data should be formatted as **tensors**.\n",
    "    - the values taken by these tensors should usually be scaled to small values (-1,1) or (0,1)\n",
    "    - if different features take values in different ranges, then the data should be **normalized**.\n",
    "    - you may want to do some **feature engineering**, especially for **small-data problems**.\n",
    "5. **Developing a model that does better than a baseline**\n",
    "    - you need to make three key choices to build your first working model\n",
    "    \n",
    "    <img width=\"600\" alt=\"creating a repo\" src=\"https://drive.google.com/uc?export=view&id=1Td3XWIeYStS600Swrp0G-Brne9P-BrAw\">\n",
    "\n",
    "6. **Scaling up: developing a model that overfits**\n",
    "    - add layers.\n",
    "    - make the layers bigger.\n",
    "    - train for more epochs.\n",
    "    - note: when you see that the model’s performance on the validation data begins to degrade, you’ve achieved overfitting\n",
    "    \n",
    "7. **Regularizing your model and tuning your hyperparameters**\n",
    "    - Add dropout.\n",
    "    - Try different architectures: add or remove layers.\n",
    "    - Add L1 and/or L2 regularization\n",
    "    - Try different hyperparameters (such as the number of units per layer or the learning rate of the optimizer) to find the optimal configuration.\n",
    "    - Optionally, iterate on feature engineering: add new features, or remove features that don’t seem to be informative."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
