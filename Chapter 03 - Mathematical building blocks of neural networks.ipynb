{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "This chapter covers\n",
    "- A first example of a neural network\n",
    "- Tensors and tensor operations\n",
    "- How neural networks learn via backpropagation and gradient descent\n",
    "\n",
    "**Understanding deep learning** requires familiarity with many simple mathematical\n",
    "concepts: **tensors**, **tensor operations**, **differentiation**, **gradient descent**, and so on.\n",
    "Our goal in this chapter will be to build your intuition about these notions without\n",
    "getting overly technical. In particular, we’ll steer away from mathematical notation,\n",
    "which can be off-putting for those without any mathematics background and isn’t\n",
    "strictly necessary to explain things well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. A first look at a neural network\n",
    "\n",
    "Let’s look at a concrete example of a neural network that uses the [Python library Keras](https://keras.io/) to learn to **classify handwritten digits**. Unless you already have experience with Keras or similar libraries, you won’t understand everything about this first example right away.\n",
    "\n",
    "You probably haven’t even installed Keras yet (you can install in your machine even if it doesnt have a GPU):\n",
    "\n",
    ">```bash\n",
    "conda install -c conda-forge tensorflow keras\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ivanovitch/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.5\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "print(keras.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem we’re trying to solve here is to **classify grayscale images of handwritten digits (28 × 28 pixels)** into their **10 categories (0 through 9)**. We’ll use the [MNIST dataset](http://yann.lecun.com/exdb/mnist/), a classic in the machine-learning community, which has been around almost as long as the field itself and has been intensively studied. \n",
    "\n",
    "It’s a **set of 60,000 training images**, plus **10,000 test images**, assembled by the National Institute of Standards and Technology (the NIST in MNIST) in the 1980s. You can think of **“solving” MNIST as the “Hello World” of deep learning** — it’s what you do to verify that your algorithms are working as expected. As you become a machine-learning practitioner, you’ll see MNIST come up over and over again, in scientific papers, blog posts, and so on.\n",
    "\n",
    "<img width=\"600\" alt=\"creating a repo\" src=\"https://drive.google.com/uc?export=view&id=1lH1e-555mRrwWOMRJmu_1_Ubr0b9TQZK\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the MNIST dataset in Keras\n",
    "from keras.datasets import mnist\n",
    "\n",
    "# The images are encoded as Numpy arrays, and the labels are an array of digits, ranging\n",
    "# from 0 to 9. The images and labels have a one-to-one correspondence.\n",
    "\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of train_images <class 'numpy.ndarray'>\n",
      "Type of train_labels <class 'numpy.ndarray'>\n",
      "\n",
      "Shape of train data: images (60000, 28, 28), labels (60000,)\n",
      "Shape of test data: images (10000, 28, 28), labels (10000,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Type of train_images {}\\nType of train_labels {}\".format(type(train_images),type(train_labels)))\n",
    "print(\"\\nShape of train data: images {}, labels {}\".format(train_images.shape,train_labels.shape))\n",
    "print(\"Shape of test data: images {}, labels {}\".format(test_images.shape,test_labels.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 The network architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "# The network architecture\n",
    "network = models.Sequential()\n",
    "\n",
    "# Input layer\n",
    "network.add(layers.Dense(512, activation='relu', input_shape=(28 * 28,)))\n",
    "\n",
    "# Output layer\n",
    "network.add(layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make the network ready for training, we need to pick three more things, as part of the compilation step:\n",
    "- **A loss function**: How the network will be able to measure its performance on the training data, and thus how it will be able to steer itself in the right direction.\n",
    "- **An optimizer**: The mechanism through which the network will update itself based on the data it sees and its loss function.\n",
    "- **Metrics to monitor during training and testing**: Here, we’ll only care about accuracy (the fraction of the images that were correctly classified)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 The compilation step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "network.compile(optimizer='rmsprop',\n",
    "                loss='categorical_crossentropy',\n",
    "                metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Preparing data\n",
    "\n",
    "Before training, we’ll preprocess the data by **reshaping** it into the shape the network expects and scaling it so that **all values are in the [0, 1] interval**. Previously, our training images, for instance, were stored in an array of shape **(60000, 28, 28)** of type uint8 with values in the [0, 255] interval. We transform it into a float32 array of shape **(60000, 28 * 28)** with values between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = train_images.reshape((60000, 28 * 28))\n",
    "train_images = train_images.astype('float32') / 255\n",
    "\n",
    "test_images = test_images.reshape((10000, 28 * 28))\n",
    "test_images = test_images.astype('float32') / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Preparing the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "train_labels = to_categorical(train_labels)\n",
    "test_labels = to_categorical(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 7s 121us/step - loss: 0.2553 - acc: 0.9267\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 7s 121us/step - loss: 0.1021 - acc: 0.9696\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 7s 122us/step - loss: 0.0676 - acc: 0.9800\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 7s 122us/step - loss: 0.0495 - acc: 0.9848\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 7s 123us/step - loss: 0.0364 - acc: 0.9891\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x107b5e978>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network.fit(train_images, train_labels, epochs=5, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two quantities are displayed during training: the **loss of the network** over the training data, and the **accuracy of the network** over the training data. We quickly reach an accuracy of 0.9891 (98.9%) on the training data. Now let’s\n",
    "check that the model performs well on the test set, too:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7 Evaluate the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test-set accuracy turns out to be **97.8%** — that’s quite a bit lower than the training set accuracy. This gap between training accuracy and test accuracy is an example of **overfitting**: the fact that machine-learning models tend to perform worse on new data than on their training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1s 65us/step\n",
      "test_acc: 0.9817\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = network.evaluate(test_images, test_labels)\n",
    "print('test_acc:', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.8 Summarization\n",
    "\n",
    "This concludes our first example—you just saw how you can build and train a neural network to classify handwritten digits in less than 20 lines of Python code. The follow steps were performed:\n",
    "\n",
    "- Loading the data\n",
    "- Create the network architecture\n",
    "- Compilation\n",
    "- Preparing the data\n",
    "- Train the network \n",
    "- Evaluate the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data representations for neural networks\n",
    "\n",
    "In the previous example, we started from data stored in **multidimensional Numpy arrays**, also called **tensors**. \n",
    "\n",
    "So what’s a tensor?\n",
    "- At its core, a **tensor is a container for data** — almost always numerical data. So, it’s a\n",
    "container for numbers.\n",
    "- You may be already familiar with **matrices, which are 2D tensors**.\n",
    "- Tensors are a generalization of matrices to an arbitrary number of dimensions (note that in the context of tensors, a **dimension is often called an axis**)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0D tensor (dimension): 0\n",
      "1D tensor (dimension): 1\n",
      "2D tensor (dimension): 2\n",
      "3D tensor (dimension): 3\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Scalars (0D tensors) - a tensor that contains only one number is called a scalar\n",
    "scalar = np.array(12)\n",
    "print(\"0D tensor (dimension): {}\".format(scalar.ndim))\n",
    "\n",
    "# Vectors (1D tensors) - an array of numbers is called a vector, or 1D tensor. A 1D tensor is said to have exactly one axis.\n",
    "tensor1D = np.array([12, 3, 6, 14])\n",
    "print(\"1D tensor (dimension): {}\".format(tensor1D.ndim))\n",
    "\n",
    "# Matrices (2D tensors) - an array of vectors is a matrix, or 2D tensor. A matrix has two axes (rows and columns).\n",
    "tensor2D = np.array([[5, 78, 2, 34, 0],[6, 79, 3, 35, 1],[7, 80, 4, 36, 2]])\n",
    "print(\"2D tensor (dimension): {}\".format(tensor2D.ndim))\n",
    "\n",
    "# 3D tensors and higher-dimensional tensors\n",
    "# If you pack such matrices in a new array, you obtain a 3D tensor, which you can visually interpret as a cube of numbers.\n",
    "tensor3D = np.array([[[5, 78, 2, 34, 0],[6, 79, 3, 35, 1],[7, 80, 4, 36, 2]],\n",
    "                      [[5, 78, 2, 34, 0],[6, 79, 3, 35, 1],[7, 80, 4, 36, 2]],\n",
    "                      [[5, 78, 2, 34, 0],[6, 79, 3, 35, 1],[7, 80, 4, 36, 2]]])\n",
    "print(\"3D tensor (dimension): {}\".format(tensor3D.ndim))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Real-world examples of data tensors\n",
    "\n",
    "Let’s make data tensors more concrete with a few examples similar to what you’ll encounter later. The data you’ll manipulate will almost always fall into one of the following categories:\n",
    "\n",
    "- **Vector**: data—2D tensors of shape **(samples, features)**\n",
    "- **Timeseries**: data or sequence data—3D tensors of shape **(samples, timesteps,features)**\n",
    "- **Images**: 4D tensors of shape **(samples, height, width, channels)** or **(samples,channels, height, width)**\n",
    "- **Video**: 5D tensors of shape **(samples, frames, height, width, channels)** or **(samples, frames, channels, height, width)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. The gears of neural networks: tensor operations\n",
    "\n",
    "All transformations learned by deep neural networks can be reduced to a handful of tensor operations applied to\n",
    "tensors of numeric data. For instance, it’s possible to **add tensors, multiply tensors**, **reshape tensors**, and so on.\n",
    "\n",
    ">```python\n",
    "# this layer takes as input a 2D tensor and returns another 2D tensor\n",
    "keras.layers.Dense(512, activation='relu')\n",
    "```\n",
    "\n",
    "- Specifically, the function is as follows (where W is a 2D tensor and b is a vector, both attributes of the layer):\n",
    "\n",
    ">```python\n",
    "output = relu(dot(W, input) + b)\n",
    "```\n",
    "\n",
    "- We have three tensor operations here: \n",
    "    - a **dot product** (dot) between the input tensor and a tensor named W; \n",
    "        - Tensor product operation\n",
    "    - an **addition (+)** between the resulting 2D tensor and a vector b (1D); \n",
    "        - Broadcast operation\n",
    "    - a **relu** operation. relu(x) is max(x, 0).\n",
    "        - Element-wise operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0]\n",
      "[[1 0]\n",
      " [0 0]]\n"
     ]
    }
   ],
   "source": [
    "''' Tensor product (similar to a matrix multiplication)\n",
    "(a, b, c, d) . (d,) -> (a, b, c)\n",
    "(a, b, c, d) . (d, e) -> (a, b, c, e)\n",
    "'''\n",
    "\n",
    "tensor2D = np.array([[1,-1],[0,2]])\n",
    "tensor1D = np.array([1, 0])\n",
    "\n",
    "# Tensor product\n",
    "print(np.dot(tensor2D,tensor1D))\n",
    "\n",
    "# Element-wise operation\n",
    "print(tensor2D * tensor1D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2, -1],\n",
       "       [ 1,  2]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' Broadcast operation\n",
    "What happens with addition when the shapes of the two tensors being added differ?\n",
    "'''\n",
    "\n",
    "tensor2D = np.array([[1,-1],[0,2]])\n",
    "tensor1D = np.array([1, 0])\n",
    "\n",
    "tensor2D + tensor1D\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1 -1]\n",
      " [ 0  2]]\n",
      "[[1 0]\n",
      " [0 2]]\n"
     ]
    }
   ],
   "source": [
    "''' Element-wise operation \n",
    "tensor2D =  |1,-1|   ->   relu(tensor2D)   ->  |1,0|     \n",
    "            |0, 2|                             |0,2|\n",
    "'''\n",
    "\n",
    "tensor2D = np.array([[1,-1],[0,2]])\n",
    "# before relu()\n",
    "print(tensor2D)\n",
    "\n",
    "# after relu()\n",
    "print(np.maximum(tensor2D,0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. The engine of neural networks: gradient-based optimization\n",
    "\n",
    "As you saw in the previous section, each neural layer from our first network example transforms its input data as follows:\n",
    "\n",
    ">```python\n",
    "output = relu(dot(W, input) + b)\n",
    "```\n",
    "\n",
    "In this expression, **W** and **b** are tensors that are attributes of the layer. They’re called the **weights or trainable parameters** of the layer (**the kernel and bias attributes**, respectively). \n",
    "\n",
    "- These weights contain the **information learned** by the network from **exposure to training data**.\n",
    "- Initially, these **weight** matrices are filled with small random values (a step called **random initialization**). \n",
    "- **Gradually adjust** these weights, based on a feedback signal (also called **training**). A trainning loop:\n",
    "    1. Draw a **batch** of training samples x and corresponding targets y.\n",
    "    2. Run the network on x (a step called the **forward pass**) to **obtain predictions y_pred**.\n",
    "    3. **Compute the loss** of the network on the batch, a measure of the mismatch between y_pred and y.\n",
    "    4. **Update all weights** of the network in a way that slightly reduces the loss on this batch.\n",
    "    \n",
    "**Step 1** sounds easy enough—just I/O code. **Steps 2 and 3** are merely the application of a handful of tensor operations, so you could implement these steps purely from what you learned in the previous section. The difficult part is **step 4**: updating the network’s weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question!!!\n",
    "Given an individual weight coefficient in the network, how can you compute whether the coefficient should be increased or decreased, and by how much?\n",
    "\n",
    "**One naive solution** would be to freeze all weights in the network except the one scalar coefficient being considered, and try different values for this coefficient. \n",
    "- Let’s say the initial value of the **coefficient is 0.3**. \n",
    "- After the forward pass on a batch of data, **the loss** of the network on the batch is **0.5**. \n",
    "- If you change the **coefficient’s value to 0.35** and rerun the forward pass, the **loss increases to 0.6**. \n",
    "- But if you lower the **coefficient to 0.25**, the **loss falls to 0.4**.\n",
    "- In this case, it seems that updating the coefficient by -0.05 would contribute to minimizing the loss. \n",
    "- This would have to be repeated for all coefficients in the network.\n",
    "\n",
    "But such an approach would be horribly inefficient, because you’d need to compute two forward passes (which are expensive) for every individual coefficient (of which there are many, usually thousands and sometimes up to millions). \n",
    "\n",
    "**A much better approach** is to take advantage of the fact that **all operations** used in the network are **differentiable**, and compute the gradient of the loss with regard to the network’s coefficients. You can then **move the coefficients** in the **opposite direction from the gradient**, thus **decreasing the loss**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 What’s a derivative?\n",
    "\n",
    "Consider a continuous, smooth function $f(x)$, mapping a real number x to a new real number y. \n",
    "\n",
    "$$f(x) = y$$\n",
    "\n",
    "Because the function is continuous, a small change in $x$ can only result in a small change in $y$ — that’s the intuition behind continuity. Let’s say you increase $x$ by a small factor $epsilon\\_x$ will result in a small $epsilon\\_y$ change to $y$:\n",
    "\n",
    "$$f(x + epsilon\\_x) = y + epsilon\\_y$$\n",
    "\n",
    "In addition, because the function is *smooth* (its curve doesn’t have any abrupt angles), **when $epsilon\\_x$ is small enough**, around a certain point $p$, it’s possible to approximate $f$ as a linear function of slope $a$, so that $epsilon\\_y$ becomes $a * epsilon\\_x$:\n",
    "\n",
    "$$f(x + epsilon\\_x) = y + a * epsilon\\_x$$\n",
    "\n",
    "**The slope a** is called the **derivative** of $f$ in $p$. \n",
    "\n",
    "- If $a$ is negative, it means a small change of $x$ around $p$ will result in a decrease of $f(x)$\n",
    "- If f $a$ is positive, a small change in $x$ will result in an increase of $f(x)$. \n",
    "- Further, the absolute value of $a$ (the magnitude of the derivative) tells you how quickly this increase or decrease\n",
    "will happen.\n",
    "\n",
    "If you’re trying to update $x$ by a factor $epsilon\\_x$ in order to minimize $f(x)$, and you know the derivative of $f$, then your job is done: the derivative completely describes how $f(x)$ evolves as you change $x$. **If you want to reduce the value of $f(x)$, you just need to move $x$ a little in the opposite direction from the derivative.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Derivative of a tensor operation: the gradient\n",
    "\n",
    "A **gradient** is the **derivative of a tensor operation**. \n",
    "\n",
    "Consider:\n",
    "- input vector $x$\n",
    "- matrix $W$\n",
    "- target $y$\n",
    "- loss function. \n",
    "\n",
    "You can use $W$ to compute a target candidate $y\\_pred$, and compute the $loss$, or mismatch, between the target candidate $y\\_pred$ and the target $y$:\n",
    "\n",
    "$$\n",
    "y\\_pred = dot(W, x)\n",
    "$$\n",
    "\n",
    "$$\n",
    "loss\\_value = loss(y\\_pred, y)\n",
    "$$\n",
    "\n",
    "If the data inputs $x$ and $y$ are frozen, then this can be interpreted as a function mapping values of $W$ to loss values:\n",
    "\n",
    "$$loss\\_value = f(W)$$\n",
    "\n",
    "Let’s say the current value of $W$ is $W0$. \n",
    "\n",
    "- The derivative of $f$ in the point $W0$ is a tensor $gradient(f)(W0)$ \n",
    "- Each coefficient $gradient(f) (W0)[i, j]$ indicates the direction and magnitude of the change in $loss\\_value$\n",
    "\n",
    "\n",
    "Thus, for a function $f(x)$, you can reduce the value of $f(x)$ by moving $x$ a little in the opposite direction from the derivative, with a function f(W) of a tensor, you can reduce f(W) by moving W in the opposite direction from the gradient: \n",
    "\n",
    "$$\n",
    "W1 = W0 - step * gradient(f)(W0)\n",
    "$$\n",
    "\n",
    "where step is a small scaling factor. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Stochastic gradient descent\n",
    "\n",
    "1. Draw a batch of training samples $x$ and corresponding targets $y$.\n",
    "2. Run the network on $x$ to obtain predictions $y\\_pred$.\n",
    "3. Compute the loss of the network on the batch, a measure of the mismatch between $y\\_pred$ and $y$.\n",
    "4. Compute the gradient of the loss with regard to the network’s parameters (a backward pass).\n",
    "5. Move the parameters a little in the opposite direction from the gradient \n",
    "    - $W -= step * gradient$\n",
    "    \n",
    "What I just described is called mini-batch stochastic gradient descent.\n",
    "\n",
    "Additionally, there exist multiple variants of SGD that differ by taking into account previous weight updates when computing the next weight update, rather than just looking at the current value of the gradients. There is, for instance, **SGD with momentum**, as well as **Adagrad**, **RMSProp**, and several others. **Such variants are known as optimization methods or optimizers.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Looking back at our first example\n",
    "\n",
    "You’ve reached the end of this chapter, and you should now have a general understanding of what’s going on behind the scenes in a neural network. Let’s go back to the first example and review each piece of it in the light of what you’ve learned in the previous three sections.\n",
    "\n",
    "This was the input data:\n",
    "\n",
    ">```python\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "train_images = train_images.reshape((60000, 28 * 28))\n",
    "train_images = train_images.astype('float32') / 255\n",
    "test_images = test_images.reshape((10000, 28 * 28))\n",
    "test_images = test_images.astype('float32') / 255\n",
    "```\n",
    "\n",
    "Now you understand that the input images are stored in Numpy tensors, which are here formatted as float32 tensors of shape **(60000, 784) (training data)** and **(10000,784) (test data)**, respectively.\n",
    "\n",
    "This was our network:\n",
    "\n",
    ">```python\n",
    "network = models.Sequential()\n",
    "network.add(layers.Dense(512, activation='relu', input_shape=(28 * 28,)))\n",
    "network.add(layers.Dense(10, activation='softmax'))\n",
    "```\n",
    "\n",
    "Now you understand that this network consists of a **chain of two Dense layers**, that each layer applies a few simple tensor operations to the input data, and that these operations involve weight tensors. Weight tensors, which are attributes of the layers, are where the knowledge of the network persists.\n",
    "\n",
    "This was the network-compilation step:\n",
    "\n",
    ">```python\n",
    "network.compile(optimizer='rmsprop',\n",
    "loss='categorical_crossentropy',\n",
    "metrics=['accuracy'])\n",
    "```\n",
    "\n",
    "Now you understand that **categorical_crossentropy** is the loss function that’s used as a feedback signal for learning the weight tensors, and which the training phase will attempt to minimize. You also know that this **reduction of the loss happens via minibatch stochastic gradient descent**. The exact rules governing a specific use of gradient descent are defined by the **rmsprop optimizer** passed as the first argument.\n",
    "\n",
    "Finally, this was the training loop:\n",
    "\n",
    ">```python\n",
    "network.fit(train_images, train_labels, epochs=5, batch_size=128)\n",
    "```\n",
    "\n",
    "Now you understand what happens when you call fit: the network will start to iterate on the training data in mini-batches of **128 samples, 5 times over** (each iteration over all the training data is called an epoch). At each iteration, the network will compute the gradients of the weights with regard to the loss on the batch, and update the weights accordingly. **After these 5 epochs, the network will have performed 2,345 gradient updates (469 per epoch)**, and the loss of the network will be sufficiently low that the network will be capable of classifying handwritten digits with high accuracy.\n",
    "\n",
    "**At this point, you already know most of what there is to know about neural networks.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
