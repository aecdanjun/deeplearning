{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "This chapter covers\n",
    "- **Core** components of **neural networks**\n",
    "- An introduction to **Keras**\n",
    "- Using neural networks to solve basic classification and regression problems\n",
    "    - Classifying movie reviews as positive or negative (**binary classification**)\n",
    "    - Classifying news wires by topic (**multiclass classification**)\n",
    "    - Estimating the price of a house, given real-estate data (**regression**)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Anatomy of a neural network\n",
    "\n",
    "As you saw in the previous chapters, training a neural network revolves around the following objects:\n",
    " - **Layers**, which are combined into a network (or model)\n",
    " - The **input data** and corresponding **targets**\n",
    " - The **loss function**, which defines the **feedback signal used for learning**\n",
    " - The **optimizer**, which determines **how learning proceeds**\n",
    " \n",
    " <img width=\"400\" alt=\"creating a repo\" src=\"https://drive.google.com/uc?export=view&id=1CFWZALw9MnJvStvh8VkCAZFiVLs7E4lW\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Layers: the building blocks of deep learning\n",
    "\n",
    "A layer is a data-processing module that takes as input one or more tensors and that outputs one or more tensors. Different layers are appropriate for different tensor formats and different types of data processing.\n",
    "\n",
    "- **fully connected or dense layers**\n",
    "    - simple vector data, stored in 2D tensors of shape\n",
    "- **recurrent layers such as an LSTM layer**\n",
    "    - Sequence data, stored in 3D tensors of shape\n",
    "- **2D convolution layers (Conv2D)**\n",
    "    - image data, stored in 4D tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Loss functions and optimizers: keys to configuring the learning process\n",
    "\n",
    "Once the network architecture is defined, you still have to choose two more things:\n",
    "- **Loss function** (objective function) — The quantity that will be minimized during training. It represents a measure of success for the task at hand.\n",
    "- **Optimizer** — Determines how the network will be updated based on the loss function. It implements a specific variant of stochastic gradient descent (SGD).\n",
    "\n",
    "**A neural network that has multiple outputs may have multiple loss functions (one per output)**. But the gradient-descent process must be based on a single scalar loss value; so, for multiloss networks, all losses are combined (via averaging) into a single scalar quantity.\n",
    "\n",
    "Choosing the right objective function for the right problem is extremely important.\n",
    "\n",
    "- **binary crossentropy**\n",
    "    - two-class classification\n",
    "- **categorical crossentropy**\n",
    "    - many-class classification problem\n",
    "- **mean squared error** \n",
    "    - regression problem\n",
    "- **connectionist temporal classification (CTC)**\n",
    "    - sequence-learning problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Introduction to Keras\n",
    "\n",
    "[Keras](https://keras.io/) is a **deep-learning framework for Python** that provides a convenient way to define and train almost any kind of deep-learning model.\n",
    "\n",
    "Keras has the following key features:\n",
    "- It allows the same code to run seamlessly on CPU or GPU.\n",
    "- It has a user-friendly API that makes it easy to quickly prototype deep-learning models.\n",
    "- It has built-in support for convolutional networks (for computer vision), recurrent\n",
    "networks (for sequence processing), and any combination of both.\n",
    "- It supports arbitrary network architectures: multi-input or multi-output models, layer sharing, model sharing, and so on. \n",
    "\n",
    "\n",
    "Keras is appropriate for building essentially any **deep-learning model**, from a **generative adversarial network**\n",
    "to a **neural Turing machine**.\n",
    "\n",
    "Keras is distributed under the permissive MIT license, which means **it can be freely used in commercial projects**. It’s compatible with any version of Python from 2.7 to 3.6 (as of april 2018).\n",
    "\n",
    "\n",
    "**Keras is a model-level library**, providing high-level building blocks for developing deep-learning models. **It doesn’t handle low-level operations** such as tensor manipulation and differentiation. Instead, **it relies on a specialized, well-optimized tensor library** to do so, serving as the backend engine of Keras.\n",
    "\n",
    "<img width=\"300\" alt=\"creating a repo\" src=\"https://drive.google.com/uc?export=view&id=1XeOgdVuT35U9ahTWwO5vfW6EiUlklrql\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.0 Classifying movie reviews: a binary classification example\n",
    "\n",
    "**Two-class classification**, or **binary classification**, may be the most widely applied kind of machine-learning problem. In this example, you’ll learn to classify movie reviews as positive or negative, based on the text content of the reviews.\n",
    "\n",
    "### 4.1 IMDB dataset\n",
    "\n",
    "You’ll work with the IMDB dataset: a set of 50,000 highly polarized reviews from the\n",
    "Internet Movie Database. They’re split into:\n",
    "\n",
    "- 25,000 reviews for training\n",
    "- 25,000 reviews for testing\n",
    "\n",
    "each set consisting of 50% negative and 50% positive reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ivanovitch/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/text-datasets/imdb.npz\n",
      "17465344/17464789 [==============================] - 8s 0us/step\n"
     ]
    }
   ],
   "source": [
    "# loading the IMDB dataset\n",
    "\n",
    "from keras.datasets import imdb\n",
    "\n",
    "# num_words=10000 means you’ll only keep the top 10,000 most frequently occurring words in the training data.\n",
    "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 5952, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]\n"
     ]
    }
   ],
   "source": [
    "# The variables train_data and test_data are lists of reviews; \n",
    "# each review is a list of word indices (encoding a sequence of words)\n",
    "print(train_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train_labels and test_labels are lists of 0s and 1s, where 0 stands for negative and 1 stands for positive\n",
    "train_labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9999"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Because you’re restricting yourself to the top 10,000 most frequent words, no word index will exceed 10,000\n",
    "max([max(sequence) for sequence in train_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"? big hair big boobs bad music and a giant safety pin these are the words to best describe this terrible movie i love cheesy horror movies and i've seen hundreds but this had got to be on of the worst ever made the plot is paper thin and ridiculous the acting is an abomination the script is completely laughable the best is the end showdown with the cop and how he worked out who the killer is it's just so damn terribly written the clothes are sickening and funny in equal ? the hair is big lots of boobs ? men wear those cut ? shirts that show off their ? sickening that men actually wore them and the music is just ? trash that plays over and over again in almost every scene there is trashy music boobs and ? taking away bodies and the gym still doesn't close for ? all joking aside this is a truly bad film whose only charm is to look back on the disaster that was the 80's and have a good old laugh at how bad everything was back then\""
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how to decode one sequence back to english\n",
    "\n",
    "# get all word indexes ('word': id)\n",
    "word_index = imdb.get_word_index()\n",
    "\n",
    "# get a new dict (id: 'word')\n",
    "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
    "\n",
    "# Note that the indices are offset by 3 because 0, 1, and 2 are reserved \n",
    "# indices for “padding,” “start of sequence,” and “unknown.”\n",
    "decoded_review = ' '.join([reverse_word_index.get(i - 3, '?') for i in train_data[1]])\n",
    "decoded_review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.1 Preparing the data\n",
    "\n",
    "**You can’t feed lists of integers into a neural network**. You have to turn your lists into tensors. There are two ways to do that:\n",
    "- Pad your lists so that they all have the same length, turn them into an integer tensor of shape (samples, word_indices)\n",
    "- One-hot encode your lists to turn them into vectors of 0s and 1s. This would mean, for instance, turning the sequence [3, 5] into a 10,000-dimensional vector that would be all 0s except for indices 3 and 5, which would be 1s. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding the integer sequences into a binary matrix (25000,10000)\n",
    "import numpy as np\n",
    "\n",
    "def vectorize_sequences(sequences, dimension=10000):\n",
    "    results = np.zeros((len(sequences), dimension))\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        results[i, sequence] = 1.\n",
    "    return results\n",
    "\n",
    "x_train = vectorize_sequences(train_data)\n",
    "x_test = vectorize_sequences(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 10000)\n",
      "[0. 1. 1. ... 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(x_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You should also vectorize your labels, which is straightforward\n",
    "\n",
    "y_train = np.asarray(train_labels).astype('float32')\n",
    "y_test = np.asarray(test_labels).astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.2 Building your network\n",
    "\n",
    "- The input data is vectors\n",
    "- The labels are scalars (1s and 0s)\n",
    "\n",
    "A type of network that performs well on such a problem is **a simple stack of fully connected (Dense) layers** with relu activations \n",
    "\n",
    ">```python\n",
    "Dense(16,activation='relu')\n",
    "```\n",
    "\n",
    "The argument being passed to each Dense layer (16) is the number of hidden units of the layer. **A hidden unit is a dimension in the representation space of the layer**. Remember that each such Dense layer with a relu activation implements the following chain of tensor operations:\n",
    "\n",
    ">```python\n",
    "output = relu(dot(W, input) + b)\n",
    "```\n",
    "\n",
    "Having 16 hidden units means the weight matrix W will have shape (input_dimension,16): the dot product with W will project the input data onto a 16-dimensional representation space (and then you’ll add the bias vector b and apply the relu operation). \n",
    "\n",
    "You can intuitively understand the dimensionality of your representation space as **“how much freedom you’re allowing the network to have when learning internal representations.”** \n",
    "\n",
    "- Having more hidden units (a higher-dimensional representation space) allows your network to learn more-complex representations\n",
    "- But, having more hidden units make the network more computationally expensive and may lead to **learning unwanted patterns (patterns that will improve performance on the training data but not on the test data).**\n",
    "\n",
    "\n",
    "There are two **key architecture decisions** to be made about such a stack of Dense layers:\n",
    "- How many layers to use\n",
    "- How many hidden units to choose for each layer\n",
    "\n",
    "In the next chapter, you’ll learn formal principles to guide you in making these choices. For the time being, you’ll have to trust me with the following architecture choice:\n",
    "\n",
    "- Two intermediate layers with 16 hidden units each\n",
    "- A third layer that will output the scalar prediction regarding the sentiment of the current review\n",
    "\n",
    "The **intermediate layers will use relu** as their activation function, and the **final layer will use a sigmoid** activation so as to output a probability (a score between 0 and 1).\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "    <td> <img src=\"https://drive.google.com/uc?export=view&id=1tGaZcNRpAmDaSEAOGCKGhH9QtPHlbDUi\" width=\"150\"> </td>\n",
    "    <td> <img src=\"https://drive.google.com/uc?export=view&id=1ku-YktP--VeodXH1oaUPPtkvgfSVclm7\" width=\"300\"> </td>\n",
    "    <td> <img src=\"https://drive.google.com/uc?export=view&id=1LoV52worh3m-S9uL4YvyDm2NVOcVDvaJ\" width=\"300\"> </td>\n",
    "</tr>\n",
    "</table>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "# the model definition\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(16, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(16, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "# compile the model\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.3 Validating your approach\n",
    "\n",
    "In order to monitor during training the accuracy of the model on data it has never seen before, you’ll create a validation set by setting apart 10,000 samples from the original training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting aside a validation set\n",
    "x_val = x_train[:10000]\n",
    "partial_x_train = x_train[10000:]\n",
    "\n",
    "y_val = y_train[:10000]\n",
    "partial_y_train = y_train[10000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "15000/15000 [==============================] - 5s 318us/step - loss: 0.5085 - acc: 0.7815 - val_loss: 0.3797 - val_acc: 0.8690\n",
      "Epoch 2/20\n",
      "15000/15000 [==============================] - 3s 221us/step - loss: 0.3004 - acc: 0.9045 - val_loss: 0.3002 - val_acc: 0.8900\n",
      "Epoch 3/20\n",
      "15000/15000 [==============================] - 3s 185us/step - loss: 0.2179 - acc: 0.9281 - val_loss: 0.3081 - val_acc: 0.8717\n",
      "Epoch 4/20\n",
      "15000/15000 [==============================] - 3s 170us/step - loss: 0.1750 - acc: 0.9435 - val_loss: 0.2840 - val_acc: 0.8839\n",
      "Epoch 5/20\n",
      "15000/15000 [==============================] - 3s 170us/step - loss: 0.1426 - acc: 0.9543 - val_loss: 0.2848 - val_acc: 0.8866\n",
      "Epoch 6/20\n",
      "15000/15000 [==============================] - 2s 165us/step - loss: 0.1151 - acc: 0.9651 - val_loss: 0.3145 - val_acc: 0.8777\n",
      "Epoch 7/20\n",
      "15000/15000 [==============================] - 2s 166us/step - loss: 0.0978 - acc: 0.9706 - val_loss: 0.3127 - val_acc: 0.8848\n",
      "Epoch 8/20\n",
      "15000/15000 [==============================] - 2s 166us/step - loss: 0.0806 - acc: 0.9765 - val_loss: 0.3848 - val_acc: 0.8660\n",
      "Epoch 9/20\n",
      "15000/15000 [==============================] - 2s 166us/step - loss: 0.0659 - acc: 0.9821 - val_loss: 0.3631 - val_acc: 0.8779\n",
      "Epoch 10/20\n",
      "15000/15000 [==============================] - 2s 165us/step - loss: 0.0553 - acc: 0.9854 - val_loss: 0.3843 - val_acc: 0.8790\n",
      "Epoch 11/20\n",
      "15000/15000 [==============================] - 3s 208us/step - loss: 0.0452 - acc: 0.9889 - val_loss: 0.4169 - val_acc: 0.8763\n",
      "Epoch 12/20\n",
      "15000/15000 [==============================] - 3s 167us/step - loss: 0.0383 - acc: 0.9914 - val_loss: 0.4507 - val_acc: 0.8703\n",
      "Epoch 13/20\n",
      "15000/15000 [==============================] - 4s 246us/step - loss: 0.0296 - acc: 0.9933 - val_loss: 0.4703 - val_acc: 0.8727\n",
      "Epoch 14/20\n",
      "15000/15000 [==============================] - 3s 185us/step - loss: 0.0242 - acc: 0.9949 - val_loss: 0.5029 - val_acc: 0.8722\n",
      "Epoch 15/20\n",
      "15000/15000 [==============================] - 3s 171us/step - loss: 0.0177 - acc: 0.9977 - val_loss: 0.5347 - val_acc: 0.8695\n",
      "Epoch 16/20\n",
      "15000/15000 [==============================] - 3s 172us/step - loss: 0.0164 - acc: 0.9972 - val_loss: 0.5723 - val_acc: 0.8694\n",
      "Epoch 17/20\n",
      "15000/15000 [==============================] - 3s 172us/step - loss: 0.0096 - acc: 0.9993 - val_loss: 0.6513 - val_acc: 0.8599\n",
      "Epoch 18/20\n",
      "15000/15000 [==============================] - 3s 168us/step - loss: 0.0120 - acc: 0.9974 - val_loss: 0.6447 - val_acc: 0.8683\n",
      "Epoch 19/20\n",
      "15000/15000 [==============================] - 3s 174us/step - loss: 0.0053 - acc: 0.9997 - val_loss: 0.7350 - val_acc: 0.8563\n",
      "Epoch 20/20\n",
      "15000/15000 [==============================] - 3s 176us/step - loss: 0.0104 - acc: 0.9973 - val_loss: 0.7036 - val_acc: 0.8662\n"
     ]
    }
   ],
   "source": [
    "# Training your model\n",
    "history = model.fit(partial_x_train,\n",
    "                    partial_y_train,\n",
    "                    epochs=20,\n",
    "                    batch_size=512,\n",
    "                    validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11f72f978>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting the training and validation loss\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "history_dict = history.history\n",
    "\n",
    "# trainning loss X validation loss\n",
    "loss_values = history_dict['loss']\n",
    "val_loss_values = history_dict['val_loss']\n",
    "epochs = range(1, len(loss_values) + 1)\n",
    "\n",
    "# trainning accuracy X validation accuracy\n",
    "acc_values = history_dict['acc']\n",
    "val_acc_values = history_dict['val_acc']\n",
    "\n",
    "fig = plt.figure(figsize=(14, 6))\n",
    "ax1 = fig.add_subplot(1,2,1)\n",
    "ax2 = fig.add_subplot(1,2,2)\n",
    "\n",
    "ax1.plot(epochs, loss_values, 'bo', label='Training loss')\n",
    "ax1.plot(epochs, val_loss_values, 'b', label='Validationb loss')\n",
    "ax1.set_title('Training and validation loss')\n",
    "ax1.set_xlabel('Epochs')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.legend()\n",
    "ax1.set_xticks(epochs)\n",
    "\n",
    "ax2.plot(epochs, acc_values, 'bo', label='Training acc')\n",
    "ax2.plot(epochs, val_acc_values, 'b', label='Validation acc')\n",
    "ax2.set_title('Training and validation accuracy')\n",
    "ax2.set_xlabel('Epochs')\n",
    "ax2.set_xticks(epochs)\n",
    "ax2.set_ylabel('Loss')\n",
    "ax2.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- the training loss decreases with every epoch\n",
    "- the training accuracy increases with every epoch\n",
    "\n",
    "That’s what you would expect when running gradient descent optimization — the quantity you’re trying to minimize should be less with every iteration. But that isn’t the case for the validation loss and accuracy: they seem to peak at the fourth epoch. This is an example of what we warned against earlier: a model that performs better on the training data isn’t necessarily a model that will do better on data it has never seen before. In precise terms, what you’re seeing is **overfitting**: after the second epoch, you’re overoptimizing on the training data, and you end up learning representations that are specific to the training data and don’t generalize to data outside of the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 2s 90us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.7774300403499603, 0.85032]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this approach achieves an accuracy of 85%. With state-of-the-art approaches, you should be able to get close to 95%\n",
    "results = model.evaluate(x_test, y_test)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.4 Using a trained network to generate predictions on new data\n",
    "\n",
    "After having trained a network, you’ll want to use it in a practical setting. You can generate the likelihood of reviews being positive by using the predict method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00902936],\n",
       "       [0.9999999 ],\n",
       "       [0.7757505 ],\n",
       "       ...,\n",
       "       [0.00240106],\n",
       "       [0.00890265],\n",
       "       [0.8260735 ]], dtype=float32)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model.predict_classes(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Further experiments\n",
    "\n",
    "The following experiments will help convince you that the architecture choices you’ve made are all fairly reasonable, although there’s still room for improvement:\n",
    "1. You used two hidden layers. **Try using one or three hidden layers**, and see how doing so affects validation and test accuracy.\n",
    "2. Try using layers with more **hidden units** or fewer hidden units: **32 units, 64 units**, and so on.\n",
    "3. Try using the **mse loss** function instead of binary_crossentropy.\n",
    "4. Try using the **tanh activation** (an activation that was popular in the early days of neural networks) instead of relu."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.1 Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters to be evaluate\n",
    "\n",
    "hidden_units = [16,32,64]\n",
    "activations_funct = ['relu','tanh']\n",
    "loss_funct = ['binary_crossentropy', 'mean_squared_error']\n",
    "training = []\n",
    "results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the model definition\n",
    "# layer_1,layer_2\n",
    "\n",
    "for hidden in hidden_units:\n",
    "    for activations in activations_funct:\n",
    "        for losses in loss_funct:\n",
    "            model = models.Sequential()\n",
    "            model.add(layers.Dense(hidden, activation=activations, input_shape=(10000,)))\n",
    "            model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "            # compile the model\n",
    "            model.compile(optimizer='rmsprop',loss=losses,metrics=['accuracy'])\n",
    "            \n",
    "            # Training your model\n",
    "            history = model.fit(partial_x_train,\n",
    "                                partial_y_train,\n",
    "                                epochs=20,\n",
    "                                batch_size=512,\n",
    "                                validation_data=(x_val, y_val))\n",
    "            training.append(history)\n",
    "            results.append(model.evaluate(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Model           | Hidden Unit | Loss Function | Evaluation (accuracy) |\n",
    "|-----------------|-------------|---------------|-----------------------|\n",
    "| **relu,sigmoid(1)** | **16**          | **binary_cross**  |       ** 0.8574  **             |\n",
    "| relu,sigmoid(1) | 16          | mse           |        0.8551             |\n",
    "| tanh,sigmoid(1) | 16          | binary_cross  |        0.8513               |\n",
    "| tanh,sigmoid(1) | 16          | mse           |        0.8515               |\n",
    "| relu,sigmoid(1) | 32          | binary_cross  |         0.8500              |\n",
    "| relu,sigmoid(1) | 32          | mse           |         0.8538              |\n",
    "| tanh,sigmoid(1) | 32          | binary_cross  |          0.8460             |\n",
    "| tanh,sigmoid(1) | 32          | mse           |          0.8469             |\n",
    "| relu,sigmoid(1) | 64          | binary_cross  |         0.8470              |\n",
    "| relu,sigmoid(1) | 64          | mse           |          0.8502             |\n",
    "| tanh,sigmoid(1) | 64          | binary_cross  |         0.8464              |\n",
    "| tanh,sigmoid(1) | 64          | mse           |         0.7906              |\n",
    "\n",
    "The best accuary was 0.8575. \n",
    "\n",
    "- For all results the activation function relu was better than tanh\n",
    "- 16 hidden units was a better choice when compared to 32 and 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.2 Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters to be evaluate\n",
    "\n",
    "hidden_units = [16,32,64]\n",
    "activations_funct = ['relu','tanh']\n",
    "loss_funct = ['binary_crossentropy', 'mean_squared_error']\n",
    "training = []\n",
    "results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the model definition\n",
    "# layer_1,layer_2,layer_3\n",
    "\n",
    "for hidden in hidden_units:\n",
    "    for activations in activations_funct:\n",
    "        for losses in loss_funct:\n",
    "            model = models.Sequential()\n",
    "            model.add(layers.Dense(hidden, activation=activations, input_shape=(10000,)))\n",
    "            model.add(layers.Dense(hidden, activation=activations))\n",
    "            model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "            # compile the model\n",
    "            model.compile(optimizer='rmsprop',loss=losses,metrics=['accuracy'])\n",
    "            \n",
    "            # Training your model\n",
    "            history = model.fit(partial_x_train,\n",
    "                                partial_y_train,\n",
    "                                epochs=20,\n",
    "                                batch_size=512,\n",
    "                                validation_data=(x_val, y_val))\n",
    "            training.append(history)\n",
    "            results.append(model.evaluate(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Model           | Hidden Unit | Loss Function | Evaluation (accuracy) |\n",
    "|-----------------|-------------|---------------|-----------------------|\n",
    "| relu,relu,sigmoid(1) | 16         | binary_cross  |        0.8519               |\n",
    "| relu,relu,sigmoid(1) | 16          | mse           |        0.8510             |\n",
    "| tanh,relu,sigmoid(1) | 16          | binary_cross  |        0.8433               |\n",
    "| tanh,relu,sigmoid(1) | 16          | mse           |        0.8503               |\n",
    "| relu,relu,sigmoid(1) | 32          | binary_cross  |         0.8485              |\n",
    "| relu,relu,sigmoid(1) | 32          | mse           |         0.8534              |\n",
    "| tanh,relu,sigmoid(1) | 32          | binary_cross  |          0.8458             |\n",
    "| tanh,relu,sigmoid(1) | 32          | mse           |          0.8519             |\n",
    "| relu,relu,sigmoid(1) | 64          | binary_cross  |         0.8535              |\n",
    "| **relu,relu,sigmoid(1)** | **64**          | **mse**           |          **0.8588**             |\n",
    "| tanh,relu,sigmoid(1) | 64          | binary_cross  |         0.8432              |\n",
    "| tanh,relu,sigmoid(1) | 64          | mse           |         0.8550              |\n",
    "\n",
    "The best accuary was 0.8588. \n",
    "\n",
    "- For all results the activation function relu was better than tanh\n",
    "- 64 hidden units was a better choice when compared to 16 and 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.3 Model 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters to be evaluate\n",
    "\n",
    "hidden_units = [16,32,64]\n",
    "activations_funct = ['relu','tanh']\n",
    "loss_funct = ['binary_crossentropy', 'mean_squared_error']\n",
    "training = []\n",
    "results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the model definition\n",
    "# layer_1,layer_2,layer_3, layer_4\n",
    "\n",
    "for hidden in hidden_units:\n",
    "    for activations in activations_funct:\n",
    "        for losses in loss_funct:\n",
    "            model = models.Sequential()\n",
    "            model.add(layers.Dense(hidden, activation=activations, input_shape=(10000,)))\n",
    "            model.add(layers.Dense(hidden, activation=activations))\n",
    "            model.add(layers.Dense(hidden, activation=activations))\n",
    "            model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "            # compile the model\n",
    "            model.compile(optimizer='rmsprop',loss=losses,metrics=['accuracy'])\n",
    "            \n",
    "            # Training your model\n",
    "            history = model.fit(partial_x_train,\n",
    "                                partial_y_train,\n",
    "                                epochs=20,\n",
    "                                batch_size=512,\n",
    "                                validation_data=(x_val, y_val))\n",
    "            training.append(history)\n",
    "            results.append(model.evaluate(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Model           | Hidden Unit | Loss Function | Evaluation (accuracy) |\n",
    "|-----------------|-------------|---------------|-----------------------|\n",
    "| relu,relu,relu,sigmoid(1) | 16         | binary_cross  |        0.8509               |\n",
    "| relu,relu,relu,sigmoid(1) | 16          | mse           |        0.8532             |\n",
    "| tanh,relu,relu,sigmoid(1) | 16          | binary_cross  |        0.8457               |\n",
    "| tanh,relu,relu,sigmoid(1) | 16          | mse           |        0.8530               |\n",
    "| relu,relu,relu,sigmoid(1) | 32          | binary_cross  |         0.8480              |\n",
    "| relu,relu,relu,sigmoid(1) | 32          | mse           |         0.8546              |\n",
    "| tanh,relu,relu,sigmoid(1) | 32          | binary_cross  |          0.8432             |\n",
    "| tanh,relu,relu,sigmoid(1) | 32          | mse           |          0.8514             |\n",
    "| relu,relu,relu,sigmoid(1) | 64          | binary_cross  |         0.8589              |\n",
    "| **relu,relu,relu,sigmoid(1)** | **64**          | **mse**           |          **0.8614**             |\n",
    "| tanh,relu,relu,sigmoid(1) | 64          | binary_cross  |         0.8452              |\n",
    "| tanh,relu,relu,sigmoid(1) | 64          | mse           |         0.8530              |\n",
    "\n",
    "The best accuary was 0.8614. \n",
    "\n",
    "- For all results the activation function relu was better than tanh\n",
    "- 64 hidden units was a better choice when compared to 16 and 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Wrapping up\n",
    "\n",
    "Here’s what you should take away from this example:\n",
    "- You usually need to do quite a bit of **preprocessing on your raw data** in order to be able to feed it—as tensors—into a neural network. \n",
    "- Stacks of Dense layers with **relu activations can solve a wide range of problems** and you’ll likely use them frequently.\n",
    "- In a **binary classification** problem (two output classes), your network should **end with a Dense layer with one unit and a sigmoid activation**: the output of your network should be a scalar between 0 and 1, encoding a probability.\n",
    "- The **rmsprop optimizer** is generally a good enough choice, whatever your problem. That’s one less thing for you to worry about.\n",
    "- As they get better on their training data, neural networks eventually start **overfitting** and end up obtaining increasingly worse results on data they’ve never seen before. Be sure to always monitor performance on data that is outside of\n",
    "the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Classifying newswires:a multiclass classification example\n",
    "\n",
    "In the previous section, you saw how to classify vector inputs into two mutually exclusive classes using a densely connected neural network. **But what happens when you have more than two classes?**\n",
    "\n",
    "In this section, you’ll build a network to **classify Reuters newswires into 46 mutually exclusive topics**. Because you have many classes, this problem is an instance of multiclass classification; and because each data point should be classified into only one category, the problem is more specifically an instance of **single-label, multiclass classification**. If each data point could belong to multiple categories (in this case, topics), you’d be facing a **multilabel, multiclass classification problem**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/text-datasets/reuters.npz\n",
      "2113536/2110848 [==============================] - 12s 6us/step\n"
     ]
    }
   ],
   "source": [
    "# Loading the Reuters dataset\n",
    "\n",
    "from keras.datasets import reuters\n",
    "\n",
    "(train_data, train_labels), (test_data, test_labels) = reuters.load_data(num_words=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8982\n",
      "2246\n"
     ]
    }
   ],
   "source": [
    "# 8982 train data samples\n",
    "print(len(train_data))\n",
    "\n",
    "# 2246 test data samples\n",
    "print(len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 4, 686, 867, 558, 4, 37, 38, 309, 2276, 465, 893, 3541, 114, 2902, 69, 312, 35, 15, 7, 335, 1679, 21, 25, 3675, 2, 3498, 58, 69, 68, 493, 5, 25, 465, 377, 2430, 4, 293, 1172, 739, 4379, 8, 7, 1510, 1131, 13, 899, 6, 4, 990, 309, 415, 4519, 6920, 645, 3916, 791, 5, 4379, 75, 8, 24, 10, 1311, 4677, 5, 344, 756, 7, 2, 231, 9691, 2603, 1413, 43, 509, 43, 68, 327, 5, 2, 3498, 297, 638, 73, 430, 22, 4, 580, 7, 48, 41, 30, 2, 136, 4, 344, 298, 4, 580, 40, 344, 5078, 2, 291, 1488, 10, 3148, 5, 231, 6250, 1308, 5, 8250, 7043, 21, 2, 1622, 990, 309, 415, 265, 5992, 8945, 1149, 9118, 2, 4, 344, 9691, 756, 3729, 2, 4667, 2, 3249, 28, 10, 2190, 24, 77, 41, 682, 10, 4851, 2048, 7, 4, 5540, 2926, 1598, 22, 370, 5954, 7541, 5, 54, 5232, 1685, 2916, 10, 1571, 946, 60, 51, 3249, 5249, 4, 73, 2135, 669, 4, 580, 64, 10, 4280, 6, 2, 25, 482, 35, 150, 377, 2430, 7, 10, 2, 836, 2, 4730, 6920, 5, 4379, 2, 2, 3541, 8, 4, 344, 291, 2, 298, 4228, 6, 2223, 24, 2, 41, 343, 430, 210, 6, 3498, 297, 64, 10, 2281, 455, 5, 7003, 125, 222, 17, 12]\n"
     ]
    }
   ],
   "source": [
    "# print a train data sample\n",
    "print(train_data[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/text-datasets/reuters_word_index.json\n",
      "557056/550378 [==============================] - 1s 3us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"? the farmers home administration the u s agriculture department's farm lending arm could lose about seven billion dlrs in outstanding principal on its severely ? borrowers or about one fourth of its farm loan portfolio the general accounting office gao said in remarks prepared for delivery to the senate agriculture committee brian crowley senior associate director of gao also said that a preliminary analysis of proposed changes in ? financial eligibility standards indicated as many as one half of ? borrowers who received new loans from the agency in 1986 would be ? under the proposed system the agency has proposed evaluating ? credit using a variety of financial ratios instead of relying solely on ? ability senate agriculture committee chairman patrick leahy d vt ? the proposed eligibility changes telling ? administrator ? clark at a hearing that they would mark a dramatic shift in the agency's purpose away from being farmers' lender of last resort toward becoming a big city bank but clark defended the new regulations saying the agency had a responsibility to ? its 70 billion dlr loan portfolio in a ? yet ? manner crowley of gao ? ? arm said the proposed credit ? system attempted to ensure that ? would make loans only to borrowers who had a reasonable change of repaying their debt reuter 3\""
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Decoding newswires back to text\n",
    "word_index = reuters.get_word_index()\n",
    "\n",
    "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
    "decoded_newswire = ' '.join([reverse_word_index.get(i - 3, '?') for i in train_data[3]])\n",
    "decoded_newswire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding the data\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def vectorize_sequences(sequences, dimension=10000):\n",
    "    results = np.zeros((len(sequences), dimension))\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        results[i, sequence] = 1.\n",
    "    return results\n",
    "\n",
    "x_train = vectorize_sequences(train_data)\n",
    "x_test = vectorize_sequences(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "one_hot_train_labels = to_categorical(train_labels)\n",
    "one_hot_test_labels = to_categorical(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model definition\n",
    "\n",
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(64, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(46, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two other things you should note about this architecture:\n",
    "\n",
    "- **You end the network with a Dense layer of size 46**. This means for each input sample, the network will output a 46-dimensional vector. Each entry in this vector (each dimension) will encode a different output class.\n",
    "- The **last layer uses a softmax activation**. You saw this pattern in the MNIST example. It means the network will output a probability distribution over the 46 different output classes—for every input sample, the network will produce a 46-\n",
    "dimensional output vector, where output[i] is the probability that the sample belongs to class i. **The 46 scores will sum to 1.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling the model\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The best loss function to use in this case is categorical_crossentropy**. It measures the distance between two probability distributions: here, between the probability distribution output by the network and the true distribution of the labels. By minimizing the distance between these two distributions, you train the network to output something as close as possible to the true labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting aside a validation set\n",
    "\n",
    "x_val = x_train[:1000]\n",
    "partial_x_train = x_train[1000:]\n",
    "\n",
    "y_val = one_hot_train_labels[:1000]\n",
    "partial_y_train = one_hot_train_labels[1000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7982 samples, validate on 1000 samples\n",
      "Epoch 1/20\n",
      "7982/7982 [==============================] - 3s 353us/step - loss: 2.5322 - acc: 0.4955 - val_loss: 1.7208 - val_acc: 0.6120\n",
      "Epoch 2/20\n",
      "7982/7982 [==============================] - 2s 246us/step - loss: 1.4452 - acc: 0.6879 - val_loss: 1.3459 - val_acc: 0.7060\n",
      "Epoch 3/20\n",
      "7982/7982 [==============================] - 2s 213us/step - loss: 1.0953 - acc: 0.7651 - val_loss: 1.1708 - val_acc: 0.7430\n",
      "Epoch 4/20\n",
      "7982/7982 [==============================] - 2s 214us/step - loss: 0.8697 - acc: 0.8165 - val_loss: 1.0793 - val_acc: 0.7590\n",
      "Epoch 5/20\n",
      "7982/7982 [==============================] - 2s 213us/step - loss: 0.7034 - acc: 0.8472 - val_loss: 0.9844 - val_acc: 0.7810\n",
      "Epoch 6/20\n",
      "7982/7982 [==============================] - 2s 212us/step - loss: 0.5667 - acc: 0.8802 - val_loss: 0.9411 - val_acc: 0.8040\n",
      "Epoch 7/20\n",
      "7982/7982 [==============================] - 2s 212us/step - loss: 0.4581 - acc: 0.9048 - val_loss: 0.9083 - val_acc: 0.8020\n",
      "Epoch 8/20\n",
      "7982/7982 [==============================] - 2s 212us/step - loss: 0.3695 - acc: 0.9231 - val_loss: 0.9363 - val_acc: 0.7890\n",
      "Epoch 9/20\n",
      "7982/7982 [==============================] - 2s 212us/step - loss: 0.3032 - acc: 0.9315 - val_loss: 0.8917 - val_acc: 0.8090\n",
      "Epoch 10/20\n",
      "7982/7982 [==============================] - 2s 212us/step - loss: 0.2537 - acc: 0.9414 - val_loss: 0.9071 - val_acc: 0.8110\n",
      "Epoch 11/20\n",
      "7982/7982 [==============================] - 2s 235us/step - loss: 0.2187 - acc: 0.9471 - val_loss: 0.9177 - val_acc: 0.8130\n",
      "Epoch 12/20\n",
      "7982/7982 [==============================] - 2s 213us/step - loss: 0.1873 - acc: 0.9508 - val_loss: 0.9027 - val_acc: 0.8130\n",
      "Epoch 13/20\n",
      "7982/7982 [==============================] - 2s 216us/step - loss: 0.1703 - acc: 0.9521 - val_loss: 0.9323 - val_acc: 0.8110\n",
      "Epoch 14/20\n",
      "7982/7982 [==============================] - 2s 232us/step - loss: 0.1536 - acc: 0.9554 - val_loss: 0.9689 - val_acc: 0.8050\n",
      "Epoch 15/20\n",
      "7982/7982 [==============================] - 2s 224us/step - loss: 0.1390 - acc: 0.9560 - val_loss: 0.9686 - val_acc: 0.8150\n",
      "Epoch 16/20\n",
      "7982/7982 [==============================] - 2s 225us/step - loss: 0.1313 - acc: 0.9560 - val_loss: 1.0220 - val_acc: 0.8060\n",
      "Epoch 17/20\n",
      "7982/7982 [==============================] - 2s 216us/step - loss: 0.1217 - acc: 0.9579 - val_loss: 1.0254 - val_acc: 0.7970\n",
      "Epoch 18/20\n",
      "7982/7982 [==============================] - 2s 229us/step - loss: 0.1198 - acc: 0.9582 - val_loss: 1.0430 - val_acc: 0.8060\n",
      "Epoch 19/20\n",
      "7982/7982 [==============================] - 2s 217us/step - loss: 0.1138 - acc: 0.9597 - val_loss: 1.0955 - val_acc: 0.7970\n",
      "Epoch 20/20\n",
      "7982/7982 [==============================] - 2s 215us/step - loss: 0.1111 - acc: 0.9593 - val_loss: 1.0674 - val_acc: 0.8020\n"
     ]
    }
   ],
   "source": [
    "# Training the model\n",
    "\n",
    "history = model.fit(partial_x_train,\n",
    "                    partial_y_train,\n",
    "                    epochs=20,\n",
    "                    batch_size=512,\n",
    "                    validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11f72f518>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting the training and validation loss\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "history_dict = history.history\n",
    "\n",
    "# trainning loss X validation loss\n",
    "loss_values = history_dict['loss']\n",
    "val_loss_values = history_dict['val_loss']\n",
    "epochs = range(1, len(loss_values) + 1)\n",
    "\n",
    "# trainning accuracy X validation accuracy\n",
    "acc_values = history_dict['acc']\n",
    "val_acc_values = history_dict['val_acc']\n",
    "\n",
    "fig = plt.figure(figsize=(14, 6))\n",
    "ax1 = fig.add_subplot(1,2,1)\n",
    "ax2 = fig.add_subplot(1,2,2)\n",
    "\n",
    "ax1.plot(epochs, loss_values, 'bo', label='Training loss')\n",
    "ax1.plot(epochs, val_loss_values, 'b', label='Validationb loss')\n",
    "ax1.set_title('Training and validation loss')\n",
    "ax1.set_xlabel('Epochs')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.legend()\n",
    "ax1.set_xticks(epochs)\n",
    "\n",
    "ax2.plot(epochs, acc_values, 'bo', label='Training acc')\n",
    "ax2.plot(epochs, val_acc_values, 'b', label='Validation acc')\n",
    "ax2.set_title('Training and validation accuracy')\n",
    "ax2.set_xlabel('Epochs')\n",
    "ax2.set_xticks(epochs)\n",
    "ax2.set_ylabel('Loss')\n",
    "ax2.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2246/2246 [==============================] - 1s 314us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.2092602904736731, 0.778717720444884]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluating the model\n",
    "model.evaluate(x_test,one_hot_test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(46,)\n",
      "1.0\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "# Generating predictions for new data\n",
    "predictions = model.predict(x_test)\n",
    "\n",
    "# Each entry in predictions is a vector of length 46\n",
    "print(predictions[0].shape)\n",
    "\n",
    "# The coefficients in this vector sum to 1\n",
    "print(np.sum(predictions[0]))\n",
    "\n",
    "# The largest entry is the predicted class—the class with the highest probability\n",
    "print(np.argmax(predictions[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 A different way to handle the labels and the loss\n",
    "\n",
    "We mentioned earlier that another way to encode the labels would be to cast them as an integer tensor, like this:\n",
    "\n",
    ">```python\n",
    "y_train = np.array(train_labels)\n",
    "y_test = np.array(test_labels)\n",
    "```\n",
    "\n",
    "The only thing this approach would change is the choice of the loss function. The loss function categorical_crossentropy, expects the labels to follow a categorical encoding. With integer labels, you should use sparse_categorical_crossentropy.\n",
    "\n",
    ">```python\n",
    "model.compile(optimizer='rmsprop',\n",
    "            loss='sparse_categorical_crossentropy',\n",
    "            metrics=['acc'])\n",
    "```\n",
    "\n",
    "This new loss function is still mathematically the same as categorical_crossentropy; it just has a different interface."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 The importance of having sufficiently large intermediate layers\n",
    "\n",
    "We mentioned earlier that because the final outputs are 46-dimensional, you should avoid intermediate layers with many fewer than 46 hidden units. Now let’s see what happens when you introduce an information bottleneck by having intermediate layers\n",
    "that are significantly less than 46-dimensional: for example, 4-dimensional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7982 samples, validate on 1000 samples\n",
      "Epoch 1/20\n",
      "7982/7982 [==============================] - 2s 288us/step - loss: 3.6391 - acc: 0.0253 - val_loss: 3.4292 - val_acc: 0.0370\n",
      "Epoch 2/20\n",
      "7982/7982 [==============================] - 2s 202us/step - loss: 3.2606 - acc: 0.0411 - val_loss: 3.0981 - val_acc: 0.0410\n",
      "Epoch 3/20\n",
      "7982/7982 [==============================] - 2s 200us/step - loss: 2.8813 - acc: 0.0614 - val_loss: 2.7139 - val_acc: 0.1840\n",
      "Epoch 4/20\n",
      "7982/7982 [==============================] - 2s 202us/step - loss: 2.4411 - acc: 0.3711 - val_loss: 2.3154 - val_acc: 0.5850\n",
      "Epoch 5/20\n",
      "7982/7982 [==============================] - 2s 204us/step - loss: 2.0331 - acc: 0.6210 - val_loss: 1.9915 - val_acc: 0.6110\n",
      "Epoch 6/20\n",
      "7982/7982 [==============================] - 2s 202us/step - loss: 1.7275 - acc: 0.6339 - val_loss: 1.7804 - val_acc: 0.6200\n",
      "Epoch 7/20\n",
      "7982/7982 [==============================] - 2s 203us/step - loss: 1.5299 - acc: 0.6423 - val_loss: 1.6590 - val_acc: 0.6210\n",
      "Epoch 8/20\n",
      "7982/7982 [==============================] - 2s 214us/step - loss: 1.3937 - acc: 0.6473 - val_loss: 1.5799 - val_acc: 0.6200\n",
      "Epoch 9/20\n",
      "7982/7982 [==============================] - 2s 227us/step - loss: 1.2920 - acc: 0.6555 - val_loss: 1.5309 - val_acc: 0.6290\n",
      "Epoch 10/20\n",
      "7982/7982 [==============================] - 2s 220us/step - loss: 1.2040 - acc: 0.6768 - val_loss: 1.4899 - val_acc: 0.6480\n",
      "Epoch 11/20\n",
      "7982/7982 [==============================] - 2s 206us/step - loss: 1.1268 - acc: 0.6978 - val_loss: 1.4690 - val_acc: 0.6570\n",
      "Epoch 12/20\n",
      "7982/7982 [==============================] - 2s 205us/step - loss: 1.0618 - acc: 0.7107 - val_loss: 1.4378 - val_acc: 0.6610\n",
      "Epoch 13/20\n",
      "7982/7982 [==============================] - 2s 206us/step - loss: 1.0038 - acc: 0.7182 - val_loss: 1.4324 - val_acc: 0.6630\n",
      "Epoch 14/20\n",
      "7982/7982 [==============================] - 2s 229us/step - loss: 0.9499 - acc: 0.7289 - val_loss: 1.4230 - val_acc: 0.6650\n",
      "Epoch 15/20\n",
      "7982/7982 [==============================] - 2s 218us/step - loss: 0.9031 - acc: 0.7393 - val_loss: 1.4257 - val_acc: 0.6660\n",
      "Epoch 16/20\n",
      "7982/7982 [==============================] - 2s 210us/step - loss: 0.8570 - acc: 0.7517 - val_loss: 1.4173 - val_acc: 0.6690\n",
      "Epoch 17/20\n",
      "7982/7982 [==============================] - 2s 206us/step - loss: 0.8149 - acc: 0.7651 - val_loss: 1.4438 - val_acc: 0.6760\n",
      "Epoch 18/20\n",
      "7982/7982 [==============================] - 2s 209us/step - loss: 0.7757 - acc: 0.7853 - val_loss: 1.4429 - val_acc: 0.6800\n",
      "Epoch 19/20\n",
      "7982/7982 [==============================] - 2s 209us/step - loss: 0.7432 - acc: 0.8051 - val_loss: 1.4448 - val_acc: 0.6910\n",
      "Epoch 20/20\n",
      "7982/7982 [==============================] - 2s 207us/step - loss: 0.7106 - acc: 0.8163 - val_loss: 1.4403 - val_acc: 0.6990\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x12d0b6470>"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(64, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(4, activation='relu'))\n",
    "model.add(layers.Dense(46, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(partial_x_train,\n",
    "          partial_y_train,\n",
    "          epochs=20,\n",
    "          batch_size=512,\n",
    "          validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2246/2246 [==============================] - 1s 340us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.8839720140475835, 0.685218165680859]"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluating the model\n",
    "model.evaluate(x_test,one_hot_test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The network now peaks at ~70% validation accuracy, an 8% absolute drop**. This drop is mostly due to the fact that you’re trying to compress a lot of information (enough information to recover the separation hyperplanes of 46 classes) into an intermediate space that is too low-dimensional."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Further experiments\n",
    "\n",
    "- Try using larger or smaller layers: 32 units, 128 units, 256, and so on.\n",
    "- You used two hidden layers. Now try using a single hidden layer, three hidden layers or four hidden layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters to be evaluate\n",
    "\n",
    "hidden_units = [32,64,128,256]\n",
    "activations_funct = ['relu']\n",
    "loss_funct = ['categorical_crossentropy']\n",
    "training = []\n",
    "results = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3.1 Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7982 samples, validate on 1000 samples\n",
      "Epoch 1/20\n",
      "7982/7982 [==============================] - 2s 230us/step - loss: 3.0657 - acc: 0.4982 - val_loss: 2.3332 - val_acc: 0.6180\n",
      "Epoch 2/20\n",
      "7982/7982 [==============================] - 1s 165us/step - loss: 1.8983 - acc: 0.6625 - val_loss: 1.6828 - val_acc: 0.6620\n",
      "Epoch 3/20\n",
      "7982/7982 [==============================] - 1s 156us/step - loss: 1.4131 - acc: 0.7111 - val_loss: 1.4089 - val_acc: 0.6970\n",
      "Epoch 4/20\n",
      "7982/7982 [==============================] - 1s 158us/step - loss: 1.1606 - acc: 0.7506 - val_loss: 1.2709 - val_acc: 0.7190\n",
      "Epoch 5/20\n",
      "7982/7982 [==============================] - 1s 162us/step - loss: 0.9895 - acc: 0.7858 - val_loss: 1.1869 - val_acc: 0.7380\n",
      "Epoch 6/20\n",
      "7982/7982 [==============================] - 1s 164us/step - loss: 0.8551 - acc: 0.8181 - val_loss: 1.1276 - val_acc: 0.7480\n",
      "Epoch 7/20\n",
      "7982/7982 [==============================] - 1s 162us/step - loss: 0.7450 - acc: 0.8409 - val_loss: 1.0611 - val_acc: 0.7770\n",
      "Epoch 8/20\n",
      "7982/7982 [==============================] - 1s 163us/step - loss: 0.6490 - acc: 0.8593 - val_loss: 1.0245 - val_acc: 0.7900\n",
      "Epoch 9/20\n",
      "7982/7982 [==============================] - 1s 158us/step - loss: 0.5665 - acc: 0.8797 - val_loss: 1.0169 - val_acc: 0.7810\n",
      "Epoch 10/20\n",
      "7982/7982 [==============================] - 1s 161us/step - loss: 0.4934 - acc: 0.8969 - val_loss: 0.9872 - val_acc: 0.7960\n",
      "Epoch 11/20\n",
      "7982/7982 [==============================] - 1s 179us/step - loss: 0.4314 - acc: 0.9109 - val_loss: 0.9724 - val_acc: 0.8040\n",
      "Epoch 12/20\n",
      "7982/7982 [==============================] - 1s 166us/step - loss: 0.3781 - acc: 0.9209 - val_loss: 0.9747 - val_acc: 0.7960\n",
      "Epoch 13/20\n",
      "7982/7982 [==============================] - 1s 163us/step - loss: 0.3344 - acc: 0.9293 - val_loss: 0.9635 - val_acc: 0.8080\n",
      "Epoch 14/20\n",
      "7982/7982 [==============================] - 1s 187us/step - loss: 0.2965 - acc: 0.9372 - val_loss: 0.9782 - val_acc: 0.8020\n",
      "Epoch 15/20\n",
      "7982/7982 [==============================] - 1s 171us/step - loss: 0.2655 - acc: 0.9435 - val_loss: 0.9893 - val_acc: 0.7950\n",
      "Epoch 16/20\n",
      "7982/7982 [==============================] - 1s 166us/step - loss: 0.2386 - acc: 0.9465 - val_loss: 0.9943 - val_acc: 0.8030\n",
      "Epoch 17/20\n",
      "7982/7982 [==============================] - 1s 167us/step - loss: 0.2156 - acc: 0.9495 - val_loss: 1.0102 - val_acc: 0.7960\n",
      "Epoch 18/20\n",
      "7982/7982 [==============================] - 1s 166us/step - loss: 0.1987 - acc: 0.9513 - val_loss: 1.0333 - val_acc: 0.7920\n",
      "Epoch 19/20\n",
      "7982/7982 [==============================] - 1s 166us/step - loss: 0.1822 - acc: 0.9496 - val_loss: 1.0431 - val_acc: 0.7930\n",
      "Epoch 20/20\n",
      "7982/7982 [==============================] - 1s 163us/step - loss: 0.1667 - acc: 0.9535 - val_loss: 1.0735 - val_acc: 0.7800\n",
      "2246/2246 [==============================] - 0s 162us/step\n",
      "Train on 7982 samples, validate on 1000 samples\n",
      "Epoch 1/20\n",
      "7982/7982 [==============================] - 2s 290us/step - loss: 2.4812 - acc: 0.5306 - val_loss: 1.6604 - val_acc: 0.6530\n",
      "Epoch 2/20\n",
      "7982/7982 [==============================] - 2s 234us/step - loss: 1.3836 - acc: 0.7066 - val_loss: 1.3036 - val_acc: 0.7120\n",
      "Epoch 3/20\n",
      "7982/7982 [==============================] - 2s 221us/step - loss: 1.0502 - acc: 0.7704 - val_loss: 1.1359 - val_acc: 0.7470\n",
      "Epoch 4/20\n",
      "7982/7982 [==============================] - 2s 245us/step - loss: 0.8424 - acc: 0.8177 - val_loss: 1.0424 - val_acc: 0.7770\n",
      "Epoch 5/20\n",
      "7982/7982 [==============================] - 2s 195us/step - loss: 0.6776 - acc: 0.8549 - val_loss: 0.9784 - val_acc: 0.8050\n",
      "Epoch 6/20\n",
      "7982/7982 [==============================] - 2s 256us/step - loss: 0.5440 - acc: 0.8852 - val_loss: 0.9385 - val_acc: 0.8100\n",
      "Epoch 7/20\n",
      "7982/7982 [==============================] - 2s 272us/step - loss: 0.4369 - acc: 0.9072 - val_loss: 0.9095 - val_acc: 0.8120\n",
      "Epoch 8/20\n",
      "7982/7982 [==============================] - 2s 269us/step - loss: 0.3567 - acc: 0.9233 - val_loss: 0.9403 - val_acc: 0.8040\n",
      "Epoch 9/20\n",
      "7982/7982 [==============================] - 2s 275us/step - loss: 0.2902 - acc: 0.9381 - val_loss: 0.9155 - val_acc: 0.8050\n",
      "Epoch 10/20\n",
      "7982/7982 [==============================] - 2s 251us/step - loss: 0.2493 - acc: 0.9432 - val_loss: 0.9113 - val_acc: 0.8200\n",
      "Epoch 11/20\n",
      "7982/7982 [==============================] - 2s 277us/step - loss: 0.2141 - acc: 0.9469 - val_loss: 0.9511 - val_acc: 0.8090\n",
      "Epoch 12/20\n",
      "7982/7982 [==============================] - 2s 265us/step - loss: 0.1866 - acc: 0.9510 - val_loss: 0.9373 - val_acc: 0.8220\n",
      "Epoch 13/20\n",
      "7982/7982 [==============================] - 2s 244us/step - loss: 0.1654 - acc: 0.9539 - val_loss: 0.9264 - val_acc: 0.8170\n",
      "Epoch 14/20\n",
      "7982/7982 [==============================] - 2s 263us/step - loss: 0.1528 - acc: 0.9545 - val_loss: 0.9556 - val_acc: 0.8170\n",
      "Epoch 15/20\n",
      "7982/7982 [==============================] - 2s 235us/step - loss: 0.1360 - acc: 0.9577 - val_loss: 1.0049 - val_acc: 0.8040\n",
      "Epoch 16/20\n",
      "7982/7982 [==============================] - 2s 215us/step - loss: 0.1324 - acc: 0.9555 - val_loss: 0.9978 - val_acc: 0.8070\n",
      "Epoch 17/20\n",
      "7982/7982 [==============================] - 2s 247us/step - loss: 0.1216 - acc: 0.9573 - val_loss: 1.0145 - val_acc: 0.8090\n",
      "Epoch 18/20\n",
      "7982/7982 [==============================] - 2s 232us/step - loss: 0.1161 - acc: 0.9587 - val_loss: 1.0549 - val_acc: 0.8070\n",
      "Epoch 19/20\n",
      "7982/7982 [==============================] - 2s 261us/step - loss: 0.1191 - acc: 0.9570 - val_loss: 1.0601 - val_acc: 0.8050\n",
      "Epoch 20/20\n",
      "7982/7982 [==============================] - 2s 282us/step - loss: 0.1122 - acc: 0.9580 - val_loss: 1.0492 - val_acc: 0.7970\n",
      "2246/2246 [==============================] - 1s 258us/step\n",
      "Train on 7982 samples, validate on 1000 samples\n",
      "Epoch 1/20\n",
      "7982/7982 [==============================] - 3s 394us/step - loss: 2.1783 - acc: 0.5663 - val_loss: 1.3733 - val_acc: 0.6900\n",
      "Epoch 2/20\n",
      "7982/7982 [==============================] - 3s 321us/step - loss: 1.1082 - acc: 0.7560 - val_loss: 1.0784 - val_acc: 0.7730\n",
      "Epoch 3/20\n",
      "7982/7982 [==============================] - 3s 323us/step - loss: 0.7727 - acc: 0.8359 - val_loss: 0.9806 - val_acc: 0.7990\n",
      "Epoch 4/20\n",
      "7982/7982 [==============================] - 3s 347us/step - loss: 0.5527 - acc: 0.8862 - val_loss: 0.8992 - val_acc: 0.8120\n",
      "Epoch 5/20\n",
      "7982/7982 [==============================] - 3s 319us/step - loss: 0.4153 - acc: 0.9095 - val_loss: 0.8651 - val_acc: 0.8220\n",
      "Epoch 6/20\n",
      "7982/7982 [==============================] - 2s 313us/step - loss: 0.3029 - acc: 0.9344 - val_loss: 0.9050 - val_acc: 0.8110\n",
      "Epoch 7/20\n",
      "7982/7982 [==============================] - 2s 312us/step - loss: 0.2480 - acc: 0.9429 - val_loss: 0.9029 - val_acc: 0.8150\n",
      "Epoch 8/20\n",
      "7982/7982 [==============================] - 3s 316us/step - loss: 0.2066 - acc: 0.9475 - val_loss: 0.9099 - val_acc: 0.8090\n",
      "Epoch 9/20\n",
      "7982/7982 [==============================] - 3s 316us/step - loss: 0.1796 - acc: 0.9534 - val_loss: 0.8713 - val_acc: 0.8290\n",
      "Epoch 10/20\n",
      "7982/7982 [==============================] - 2s 310us/step - loss: 0.1608 - acc: 0.9530 - val_loss: 1.0315 - val_acc: 0.7830\n",
      "Epoch 11/20\n",
      "7982/7982 [==============================] - 3s 335us/step - loss: 0.1525 - acc: 0.9544 - val_loss: 0.9405 - val_acc: 0.8160\n",
      "Epoch 12/20\n",
      "7982/7982 [==============================] - 3s 329us/step - loss: 0.1389 - acc: 0.9554 - val_loss: 1.0260 - val_acc: 0.7950\n",
      "Epoch 13/20\n",
      "7982/7982 [==============================] - 3s 328us/step - loss: 0.1326 - acc: 0.9555 - val_loss: 1.0267 - val_acc: 0.7970\n",
      "Epoch 14/20\n",
      "7982/7982 [==============================] - 3s 352us/step - loss: 0.1267 - acc: 0.9588 - val_loss: 1.1040 - val_acc: 0.7910\n",
      "Epoch 15/20\n",
      "7982/7982 [==============================] - 3s 360us/step - loss: 0.1218 - acc: 0.9583 - val_loss: 1.0430 - val_acc: 0.7910\n",
      "Epoch 16/20\n",
      "7982/7982 [==============================] - 3s 370us/step - loss: 0.1157 - acc: 0.9578 - val_loss: 1.0941 - val_acc: 0.7910\n",
      "Epoch 17/20\n",
      "7982/7982 [==============================] - 3s 319us/step - loss: 0.1127 - acc: 0.9585 - val_loss: 1.1191 - val_acc: 0.7870\n",
      "Epoch 18/20\n",
      "7982/7982 [==============================] - 3s 406us/step - loss: 0.1099 - acc: 0.9582 - val_loss: 1.2110 - val_acc: 0.7840\n",
      "Epoch 19/20\n",
      "7982/7982 [==============================] - 3s 340us/step - loss: 0.1125 - acc: 0.9578 - val_loss: 1.0649 - val_acc: 0.8030\n",
      "Epoch 20/20\n",
      "7982/7982 [==============================] - 2s 311us/step - loss: 0.1058 - acc: 0.9572 - val_loss: 1.0260 - val_acc: 0.8030\n",
      "2246/2246 [==============================] - 0s 182us/step\n",
      "Train on 7982 samples, validate on 1000 samples\n",
      "Epoch 1/20\n",
      "7982/7982 [==============================] - 5s 587us/step - loss: 1.9605 - acc: 0.5861 - val_loss: 1.2023 - val_acc: 0.7180\n",
      "Epoch 2/20\n",
      "7982/7982 [==============================] - 4s 514us/step - loss: 0.9485 - acc: 0.7934 - val_loss: 0.9836 - val_acc: 0.7830\n",
      "Epoch 3/20\n",
      "7982/7982 [==============================] - 4s 509us/step - loss: 0.6086 - acc: 0.8658 - val_loss: 0.9419 - val_acc: 0.7920\n",
      "Epoch 4/20\n",
      "7982/7982 [==============================] - 4s 531us/step - loss: 0.3990 - acc: 0.9114 - val_loss: 0.9455 - val_acc: 0.7940\n",
      "Epoch 5/20\n",
      "7982/7982 [==============================] - 4s 550us/step - loss: 0.2863 - acc: 0.9344 - val_loss: 0.8972 - val_acc: 0.8090\n",
      "Epoch 6/20\n",
      "7982/7982 [==============================] - 4s 501us/step - loss: 0.2332 - acc: 0.9414 - val_loss: 0.8720 - val_acc: 0.8190\n",
      "Epoch 7/20\n",
      "7982/7982 [==============================] - 4s 543us/step - loss: 0.1762 - acc: 0.9511 - val_loss: 0.9415 - val_acc: 0.7900\n",
      "Epoch 8/20\n",
      "7982/7982 [==============================] - 4s 525us/step - loss: 0.1639 - acc: 0.9545 - val_loss: 0.9024 - val_acc: 0.8120\n",
      "Epoch 9/20\n",
      "7982/7982 [==============================] - 4s 510us/step - loss: 0.1498 - acc: 0.9538 - val_loss: 0.9507 - val_acc: 0.8070\n",
      "Epoch 10/20\n",
      "7982/7982 [==============================] - 4s 510us/step - loss: 0.1363 - acc: 0.9541 - val_loss: 0.9570 - val_acc: 0.8190\n",
      "Epoch 11/20\n",
      "7982/7982 [==============================] - 4s 512us/step - loss: 0.1269 - acc: 0.9558 - val_loss: 1.0052 - val_acc: 0.7990\n",
      "Epoch 12/20\n",
      "7982/7982 [==============================] - 4s 503us/step - loss: 0.1215 - acc: 0.9554 - val_loss: 0.9813 - val_acc: 0.8040\n",
      "Epoch 13/20\n",
      "7982/7982 [==============================] - 4s 503us/step - loss: 0.1164 - acc: 0.9563 - val_loss: 1.0252 - val_acc: 0.7890\n",
      "Epoch 14/20\n",
      "7982/7982 [==============================] - 4s 506us/step - loss: 0.1118 - acc: 0.9569 - val_loss: 1.0588 - val_acc: 0.7930\n",
      "Epoch 15/20\n",
      "7982/7982 [==============================] - 4s 505us/step - loss: 0.1063 - acc: 0.9572 - val_loss: 1.0036 - val_acc: 0.7990\n",
      "Epoch 16/20\n",
      "7982/7982 [==============================] - 4s 501us/step - loss: 0.1035 - acc: 0.9570 - val_loss: 0.9914 - val_acc: 0.8070\n",
      "Epoch 17/20\n",
      "7982/7982 [==============================] - 4s 502us/step - loss: 0.1008 - acc: 0.9583 - val_loss: 1.1498 - val_acc: 0.7940\n",
      "Epoch 18/20\n",
      "7982/7982 [==============================] - 4s 507us/step - loss: 0.0956 - acc: 0.9570 - val_loss: 1.0843 - val_acc: 0.7960\n",
      "Epoch 19/20\n",
      "7982/7982 [==============================] - 4s 537us/step - loss: 0.0940 - acc: 0.9575 - val_loss: 1.0991 - val_acc: 0.8030\n",
      "Epoch 20/20\n",
      "7982/7982 [==============================] - 4s 519us/step - loss: 0.0932 - acc: 0.9548 - val_loss: 1.1121 - val_acc: 0.7940\n",
      "2246/2246 [==============================] - 1s 293us/step\n"
     ]
    }
   ],
   "source": [
    "# the model definition\n",
    "# layer_1,layer_2,layer_3\n",
    "\n",
    "for hidden in hidden_units:\n",
    "    for activations in activations_funct:\n",
    "        for losses in loss_funct:\n",
    "            model = models.Sequential()\n",
    "            model.add(layers.Dense(hidden, activation=activations, input_shape=(10000,)))\n",
    "            model.add(layers.Dense(hidden, activation=activations))\n",
    "            model.add(layers.Dense(46, activation='softmax'))\n",
    "\n",
    "            # compile the model\n",
    "            model.compile(optimizer='rmsprop',loss=losses,metrics=['accuracy'])\n",
    "            \n",
    "            # Training your model\n",
    "            history = model.fit(partial_x_train,\n",
    "                                partial_y_train,\n",
    "                                epochs=20,\n",
    "                                batch_size=512,\n",
    "                                validation_data=(x_val, y_val))\n",
    "            training.append(history)\n",
    "            results.append(model.evaluate(x_test, one_hot_test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1.1569367890260311, 0.7822796082454182],\n",
       " [1.162199652630202, 0.7943009795191451],\n",
       " [1.2036228073779112, 0.7938557435971546],\n",
       " [1.2687694323667966, 0.7951914515223549]]"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3.2 Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7982 samples, validate on 1000 samples\n",
      "Epoch 1/20\n",
      "7982/7982 [==============================] - 2s 245us/step - loss: 3.1824 - acc: 0.3825 - val_loss: 2.4689 - val_acc: 0.5030\n",
      "Epoch 2/20\n",
      "7982/7982 [==============================] - 1s 160us/step - loss: 2.0757 - acc: 0.5942 - val_loss: 1.7824 - val_acc: 0.6080\n",
      "Epoch 3/20\n",
      "7982/7982 [==============================] - 1s 158us/step - loss: 1.5601 - acc: 0.6753 - val_loss: 1.4904 - val_acc: 0.6810\n",
      "Epoch 4/20\n",
      "7982/7982 [==============================] - 1s 157us/step - loss: 1.2956 - acc: 0.7184 - val_loss: 1.3285 - val_acc: 0.7030\n",
      "Epoch 5/20\n",
      "7982/7982 [==============================] - 1s 157us/step - loss: 1.1153 - acc: 0.7471 - val_loss: 1.2305 - val_acc: 0.7160\n",
      "Epoch 6/20\n",
      "7982/7982 [==============================] - 2s 193us/step - loss: 0.9719 - acc: 0.7761 - val_loss: 1.1935 - val_acc: 0.7270\n",
      "Epoch 7/20\n",
      "7982/7982 [==============================] - 1s 187us/step - loss: 0.8581 - acc: 0.8039 - val_loss: 1.1256 - val_acc: 0.7480\n",
      "Epoch 8/20\n",
      "7982/7982 [==============================] - 2s 193us/step - loss: 0.7493 - acc: 0.8315 - val_loss: 1.1011 - val_acc: 0.7600\n",
      "Epoch 9/20\n",
      "7982/7982 [==============================] - 1s 185us/step - loss: 0.6578 - acc: 0.8540 - val_loss: 1.0899 - val_acc: 0.7620\n",
      "Epoch 10/20\n",
      "7982/7982 [==============================] - 1s 175us/step - loss: 0.5788 - acc: 0.8755 - val_loss: 1.0803 - val_acc: 0.7600\n",
      "Epoch 11/20\n",
      "7982/7982 [==============================] - 1s 161us/step - loss: 0.5058 - acc: 0.8911 - val_loss: 1.0447 - val_acc: 0.7810\n",
      "Epoch 12/20\n",
      "7982/7982 [==============================] - 1s 153us/step - loss: 0.4470 - acc: 0.9084 - val_loss: 1.0634 - val_acc: 0.7650\n",
      "Epoch 13/20\n",
      "7982/7982 [==============================] - 1s 158us/step - loss: 0.3893 - acc: 0.9201 - val_loss: 1.0524 - val_acc: 0.7830\n",
      "Epoch 14/20\n",
      "7982/7982 [==============================] - 1s 154us/step - loss: 0.3503 - acc: 0.9277 - val_loss: 1.0892 - val_acc: 0.7750\n",
      "Epoch 15/20\n",
      "7982/7982 [==============================] - 1s 175us/step - loss: 0.3119 - acc: 0.9327 - val_loss: 1.0663 - val_acc: 0.7760\n",
      "Epoch 16/20\n",
      "7982/7982 [==============================] - 2s 193us/step - loss: 0.2750 - acc: 0.9395 - val_loss: 1.1057 - val_acc: 0.7720\n",
      "Epoch 17/20\n",
      "7982/7982 [==============================] - 1s 181us/step - loss: 0.2537 - acc: 0.9412 - val_loss: 1.1014 - val_acc: 0.7760\n",
      "Epoch 18/20\n",
      "7982/7982 [==============================] - 1s 167us/step - loss: 0.2305 - acc: 0.9453 - val_loss: 1.1024 - val_acc: 0.7810\n",
      "Epoch 19/20\n",
      "7982/7982 [==============================] - 1s 164us/step - loss: 0.2103 - acc: 0.9480 - val_loss: 1.1527 - val_acc: 0.7840\n",
      "Epoch 20/20\n",
      "7982/7982 [==============================] - 1s 167us/step - loss: 0.1918 - acc: 0.9519 - val_loss: 1.1565 - val_acc: 0.7750\n",
      "2246/2246 [==============================] - 0s 162us/step\n",
      "Train on 7982 samples, validate on 1000 samples\n",
      "Epoch 1/20\n",
      "7982/7982 [==============================] - 2s 309us/step - loss: 2.8055 - acc: 0.5010 - val_loss: 1.7705 - val_acc: 0.6380\n",
      "Epoch 2/20\n",
      "7982/7982 [==============================] - 2s 235us/step - loss: 1.4740 - acc: 0.6931 - val_loss: 1.3816 - val_acc: 0.6880\n",
      "Epoch 3/20\n",
      "7982/7982 [==============================] - 2s 241us/step - loss: 1.1270 - acc: 0.7477 - val_loss: 1.1751 - val_acc: 0.7580\n",
      "Epoch 4/20\n",
      "7982/7982 [==============================] - 2s 239us/step - loss: 0.8892 - acc: 0.8039 - val_loss: 1.1023 - val_acc: 0.7460\n",
      "Epoch 5/20\n",
      "7982/7982 [==============================] - 2s 242us/step - loss: 0.7136 - acc: 0.8440 - val_loss: 1.0066 - val_acc: 0.7940\n",
      "Epoch 6/20\n",
      "7982/7982 [==============================] - 2s 233us/step - loss: 0.5802 - acc: 0.8705 - val_loss: 0.9715 - val_acc: 0.7980\n",
      "Epoch 7/20\n",
      "7982/7982 [==============================] - 2s 231us/step - loss: 0.4640 - acc: 0.8980 - val_loss: 0.9693 - val_acc: 0.8060\n",
      "Epoch 8/20\n",
      "7982/7982 [==============================] - 2s 232us/step - loss: 0.3918 - acc: 0.9164 - val_loss: 0.9534 - val_acc: 0.8140\n",
      "Epoch 9/20\n",
      "7982/7982 [==============================] - 2s 235us/step - loss: 0.3049 - acc: 0.9351 - val_loss: 0.9847 - val_acc: 0.8030\n",
      "Epoch 10/20\n",
      "7982/7982 [==============================] - 2s 235us/step - loss: 0.2585 - acc: 0.9414 - val_loss: 1.0065 - val_acc: 0.8150\n",
      "Epoch 11/20\n",
      "7982/7982 [==============================] - 2s 242us/step - loss: 0.2277 - acc: 0.9454 - val_loss: 1.0046 - val_acc: 0.8000\n",
      "Epoch 12/20\n",
      "7982/7982 [==============================] - 2s 234us/step - loss: 0.2000 - acc: 0.9485 - val_loss: 1.0785 - val_acc: 0.7960\n",
      "Epoch 13/20\n",
      "7982/7982 [==============================] - 2s 235us/step - loss: 0.1877 - acc: 0.9475 - val_loss: 1.0298 - val_acc: 0.8130\n",
      "Epoch 14/20\n",
      "7982/7982 [==============================] - 2s 244us/step - loss: 0.1668 - acc: 0.9531 - val_loss: 1.0799 - val_acc: 0.8030\n",
      "Epoch 15/20\n",
      "7982/7982 [==============================] - 2s 230us/step - loss: 0.1556 - acc: 0.9553 - val_loss: 1.1140 - val_acc: 0.7990\n",
      "Epoch 16/20\n",
      "7982/7982 [==============================] - 2s 237us/step - loss: 0.1427 - acc: 0.9573 - val_loss: 1.0925 - val_acc: 0.8020\n",
      "Epoch 17/20\n",
      "7982/7982 [==============================] - 2s 243us/step - loss: 0.1358 - acc: 0.9574 - val_loss: 1.1067 - val_acc: 0.7990\n",
      "Epoch 18/20\n",
      "7982/7982 [==============================] - 2s 236us/step - loss: 0.1306 - acc: 0.9585 - val_loss: 1.1746 - val_acc: 0.7860\n",
      "Epoch 19/20\n",
      "7982/7982 [==============================] - 2s 238us/step - loss: 0.1336 - acc: 0.9569 - val_loss: 1.1477 - val_acc: 0.7950\n",
      "Epoch 20/20\n",
      "7982/7982 [==============================] - 2s 237us/step - loss: 0.1184 - acc: 0.9599 - val_loss: 1.1618 - val_acc: 0.7910\n",
      "2246/2246 [==============================] - 1s 250us/step\n",
      "Train on 7982 samples, validate on 1000 samples\n",
      "Epoch 1/20\n",
      "7982/7982 [==============================] - 3s 425us/step - loss: 2.2064 - acc: 0.4940 - val_loss: 1.4788 - val_acc: 0.6410\n",
      "Epoch 2/20\n",
      "7982/7982 [==============================] - 3s 335us/step - loss: 1.1545 - acc: 0.7384 - val_loss: 1.2315 - val_acc: 0.7060\n",
      "Epoch 3/20\n",
      "7982/7982 [==============================] - 3s 400us/step - loss: 0.8442 - acc: 0.8028 - val_loss: 0.9992 - val_acc: 0.7730\n",
      "Epoch 4/20\n",
      "7982/7982 [==============================] - 3s 418us/step - loss: 0.5974 - acc: 0.8632 - val_loss: 0.9284 - val_acc: 0.8120\n",
      "Epoch 5/20\n",
      "7982/7982 [==============================] - 3s 346us/step - loss: 0.4470 - acc: 0.9007 - val_loss: 0.9695 - val_acc: 0.8010\n",
      "Epoch 6/20\n",
      "7982/7982 [==============================] - 3s 345us/step - loss: 0.3308 - acc: 0.9263 - val_loss: 0.9100 - val_acc: 0.8160\n",
      "Epoch 7/20\n",
      "7982/7982 [==============================] - 3s 340us/step - loss: 0.2484 - acc: 0.9425 - val_loss: 0.9741 - val_acc: 0.8070\n",
      "Epoch 8/20\n",
      "7982/7982 [==============================] - 3s 336us/step - loss: 0.2139 - acc: 0.9480 - val_loss: 1.0916 - val_acc: 0.7870\n",
      "Epoch 9/20\n",
      "7982/7982 [==============================] - 3s 334us/step - loss: 0.1859 - acc: 0.9480 - val_loss: 1.0157 - val_acc: 0.8010\n",
      "Epoch 10/20\n",
      "7982/7982 [==============================] - 3s 338us/step - loss: 0.1600 - acc: 0.9538 - val_loss: 1.1697 - val_acc: 0.7770\n",
      "Epoch 11/20\n",
      "7982/7982 [==============================] - 3s 335us/step - loss: 0.1546 - acc: 0.9563 - val_loss: 1.0757 - val_acc: 0.7950\n",
      "Epoch 12/20\n",
      "7982/7982 [==============================] - 3s 366us/step - loss: 0.1421 - acc: 0.9555 - val_loss: 1.0705 - val_acc: 0.8140\n",
      "Epoch 13/20\n",
      "7982/7982 [==============================] - 3s 336us/step - loss: 0.1293 - acc: 0.9557 - val_loss: 1.0259 - val_acc: 0.8080\n",
      "Epoch 14/20\n",
      "7982/7982 [==============================] - 3s 422us/step - loss: 0.1238 - acc: 0.9560 - val_loss: 1.0501 - val_acc: 0.8060\n",
      "Epoch 15/20\n",
      "7982/7982 [==============================] - 3s 359us/step - loss: 0.1244 - acc: 0.9554 - val_loss: 1.0260 - val_acc: 0.8100\n",
      "Epoch 16/20\n",
      "7982/7982 [==============================] - 3s 331us/step - loss: 0.1208 - acc: 0.9553 - val_loss: 1.0719 - val_acc: 0.8050\n",
      "Epoch 17/20\n",
      "7982/7982 [==============================] - 3s 342us/step - loss: 0.1043 - acc: 0.9579 - val_loss: 1.1641 - val_acc: 0.8010\n",
      "Epoch 18/20\n",
      "7982/7982 [==============================] - 3s 327us/step - loss: 0.1062 - acc: 0.9590 - val_loss: 1.1008 - val_acc: 0.8020\n",
      "Epoch 19/20\n",
      "7982/7982 [==============================] - 3s 331us/step - loss: 0.1021 - acc: 0.9587 - val_loss: 1.1747 - val_acc: 0.7980\n",
      "Epoch 20/20\n",
      "7982/7982 [==============================] - 3s 370us/step - loss: 0.1014 - acc: 0.9575 - val_loss: 1.1143 - val_acc: 0.8020\n",
      "2246/2246 [==============================] - 0s 204us/step\n",
      "Train on 7982 samples, validate on 1000 samples\n",
      "Epoch 1/20\n",
      "7982/7982 [==============================] - 5s 610us/step - loss: 2.0252 - acc: 0.5497 - val_loss: 1.2711 - val_acc: 0.7020\n",
      "Epoch 2/20\n",
      "7982/7982 [==============================] - 4s 516us/step - loss: 1.0283 - acc: 0.7663 - val_loss: 1.0195 - val_acc: 0.7760\n",
      "Epoch 3/20\n",
      "7982/7982 [==============================] - 4s 531us/step - loss: 0.7036 - acc: 0.8369 - val_loss: 0.9398 - val_acc: 0.8000\n",
      "Epoch 4/20\n",
      "7982/7982 [==============================] - 4s 561us/step - loss: 0.4764 - acc: 0.8920 - val_loss: 1.2845 - val_acc: 0.7620\n",
      "Epoch 5/20\n",
      "7982/7982 [==============================] - 4s 515us/step - loss: 0.3245 - acc: 0.9295 - val_loss: 0.9401 - val_acc: 0.8080\n",
      "Epoch 6/20\n",
      "7982/7982 [==============================] - 4s 518us/step - loss: 0.2334 - acc: 0.9440 - val_loss: 0.9515 - val_acc: 0.8160\n",
      "Epoch 7/20\n",
      "7982/7982 [==============================] - 4s 519us/step - loss: 0.1994 - acc: 0.9499 - val_loss: 1.4808 - val_acc: 0.6800\n",
      "Epoch 8/20\n",
      "7982/7982 [==============================] - 4s 528us/step - loss: 0.2079 - acc: 0.9410 - val_loss: 0.9310 - val_acc: 0.8230\n",
      "Epoch 9/20\n",
      "7982/7982 [==============================] - 4s 513us/step - loss: 0.1420 - acc: 0.9551 - val_loss: 0.9901 - val_acc: 0.8060\n",
      "Epoch 10/20\n",
      "7982/7982 [==============================] - 4s 513us/step - loss: 0.1430 - acc: 0.9545 - val_loss: 0.9904 - val_acc: 0.8040\n",
      "Epoch 11/20\n",
      "7982/7982 [==============================] - 4s 514us/step - loss: 0.1400 - acc: 0.9560 - val_loss: 0.9539 - val_acc: 0.8140\n",
      "Epoch 12/20\n",
      "7982/7982 [==============================] - 4s 511us/step - loss: 0.1232 - acc: 0.9554 - val_loss: 0.9481 - val_acc: 0.8110\n",
      "Epoch 13/20\n",
      "7982/7982 [==============================] - 4s 544us/step - loss: 0.1114 - acc: 0.9570 - val_loss: 1.1780 - val_acc: 0.7730\n",
      "Epoch 14/20\n",
      "7982/7982 [==============================] - 4s 515us/step - loss: 0.1095 - acc: 0.9563 - val_loss: 1.0995 - val_acc: 0.8030\n",
      "Epoch 15/20\n",
      "7982/7982 [==============================] - 4s 518us/step - loss: 0.1062 - acc: 0.9575 - val_loss: 1.1026 - val_acc: 0.8050\n",
      "Epoch 16/20\n",
      "7982/7982 [==============================] - 4s 513us/step - loss: 0.0983 - acc: 0.9573 - val_loss: 1.1206 - val_acc: 0.8000\n",
      "Epoch 17/20\n",
      "7982/7982 [==============================] - 4s 511us/step - loss: 0.0950 - acc: 0.9563 - val_loss: 1.1922 - val_acc: 0.7940\n",
      "Epoch 18/20\n",
      "7982/7982 [==============================] - 4s 533us/step - loss: 0.0900 - acc: 0.9593 - val_loss: 1.2530 - val_acc: 0.8000\n",
      "Epoch 19/20\n",
      "7982/7982 [==============================] - 4s 513us/step - loss: 0.0872 - acc: 0.9583 - val_loss: 1.1901 - val_acc: 0.8100\n",
      "Epoch 20/20\n",
      "7982/7982 [==============================] - 4s 509us/step - loss: 0.0843 - acc: 0.9573 - val_loss: 1.3280 - val_acc: 0.7850\n",
      "2246/2246 [==============================] - 1s 312us/step\n"
     ]
    }
   ],
   "source": [
    "# the model definition\n",
    "# layer_1,layer_2,layer_3, layer4\n",
    "\n",
    "for hidden in hidden_units:\n",
    "    for activations in activations_funct:\n",
    "        for losses in loss_funct:\n",
    "            model = models.Sequential()\n",
    "            model.add(layers.Dense(hidden, activation=activations, input_shape=(10000,)))\n",
    "            model.add(layers.Dense(hidden, activation=activations))\n",
    "            model.add(layers.Dense(hidden, activation=activations))\n",
    "            model.add(layers.Dense(46, activation='softmax'))\n",
    "\n",
    "            # compile the model\n",
    "            model.compile(optimizer='rmsprop',loss=losses,metrics=['accuracy'])\n",
    "            \n",
    "            # Training your model\n",
    "            history = model.fit(partial_x_train,\n",
    "                                partial_y_train,\n",
    "                                epochs=20,\n",
    "                                batch_size=512,\n",
    "                                validation_data=(x_val, y_val))\n",
    "            training.append(history)\n",
    "            results.append(model.evaluate(x_test, one_hot_test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1.3508902889007133, 0.7560106857164779],\n",
       " [1.2909313868117567, 0.7756010685928783],\n",
       " [1.2592158557470632, 0.7885129118963532],\n",
       " [1.5368605307564605, 0.7626892253424796]]"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[4:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 Wrapping up\n",
    "\n",
    "Here’s what you should take away from this example:\n",
    "\n",
    "1. If you’re trying to **classify data points among N classes**, your network should **end with a Dense layer of size N**.\n",
    "2. In a **single-label, multiclass classification** problem, your network should **end with a softmax** activation so that it will output a probability distribution over the N output classes.\n",
    "3. **Categorical crossentropy is almost always the loss function you should use for such problems**. It minimizes the distance between the probability distributions output by the network and the true distribution of the targets.\n",
    "4. There are two ways to handle labels in multiclass classification:\n",
    "    - Encoding the labels via **categorical encoding** (also known as one-hot encoding) and using **categorical_crossentropy** as a loss function\n",
    "    - **Encoding the labels as integers** and using the **sparse_categorical_crossentropy** loss function\n",
    "5. If you need to classify data into a large number of categories, you should avoid creating information bottlenecks in your network due to intermediate layers that are too small (**less than 46 in this example**)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Predicting house prices: a regression example\n",
    "\n",
    "The two previous examples were considered classification problems, where the goal was to predict a single discrete label of an input data point. Another common type of machine-learning problem is regression, which consists of predicting a continuous\n",
    "value instead of a discrete label: for instance, predicting the temperature tomorrow, given meteorological data; or predicting the time that a software project will take to complete, given its specifications.\n",
    "\n",
    "> Don’t confuse regression and the algorithm logistic regression. Confusingly, **logistic regression isn’t a regression algorithm—it’s a classification algorithm**.\n",
    "\n",
    "## 6.1 The Boston Housing Price dataset\n",
    "\n",
    "- You’ll attempt to predict the median price of homes in a given **Boston suburb in the mid-1970s** given data points about the suburb at the time, such as the **crime rate, the local property tax rate**, and so on. \n",
    "- The dataset has relatively **few data points**: only 506, split between 404 training samples and 102 test samples. \n",
    "- Each **feature** in the input data (for example, the crime rate) has a **different scale**. \n",
    "- For instance, some values are proportions, which take values between 0 and 1; others take values between 1 and 12, others between 0 and 100, and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(404, 13)\n",
      "(102, 13)\n"
     ]
    }
   ],
   "source": [
    "# Loading the Boston housing dataset\n",
    "\n",
    "from keras.datasets import boston_housing\n",
    "\n",
    "(train_data, train_targets), (test_data, test_targets) = boston_housing.load_data()\n",
    "\n",
    "# Training sample size\n",
    "print(train_data.shape)\n",
    "\n",
    "# Test sample size\n",
    "print(test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.0\n",
      "50.0\n"
     ]
    }
   ],
   "source": [
    "# Train targets x  U$ 1,000 - value are between 5.0 and 50.0\n",
    "print(np.min(train_targets))\n",
    "print(np.max(train_targets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1.1 Preparing the data\n",
    "\n",
    "**It would be problematic to feed into a neural network** values that all take wildly different ranges. The network might be able to automatically adapt to such heterogeneous data, but it would definitely make learning more difficult. A widespread best practice to deal with such data is to do **feature-wise normalization**: for each feature in the input data (a column in the input data matrix), you subtract the mean of the feature and divide by the standard deviation, so that the feature is centered around 0 and has a unit standard deviation. This is easily done in [Scikit-learn](http://scikit-learn.org/stable/modules/preprocessing.html).\n",
    "\n",
    "\n",
    "The [**preprocessing module**](http://scikit-learn.org/stable/modules/preprocessing.html) further provides a utility **class StandardScaler** that implements the Transformer API to compute the **mean** and **standard deviation** on a **training set** so as to be able to **later reapply the same transformation on the testing set**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.01541438e-16  1.09923072e-17  1.74337992e-15 -1.26686340e-16\n",
      " -5.25377321e-15  6.41414864e-15  2.98441140e-16  4.94653823e-16\n",
      "  1.12671149e-17 -1.98136337e-16  2.36686358e-14  5.95679996e-15\n",
      "  6.13920356e-16]\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "# Normalizing the data\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# create a scaler to fit train_data\n",
    "scaler = preprocessing.StandardScaler().fit(train_data)\n",
    "\n",
    "# feature-wise normalization over train_data\n",
    "train_data_scaled = scaler.transform(train_data)\n",
    "\n",
    "# Scaled train data has zero mean and unit variance\n",
    "print(train_data_scaled.mean(axis=0))\n",
    "print(train_data_scaled.std(axis=0))\n",
    "\n",
    "# Note that the quantities used for normalizing the test data are computed using the training data. \n",
    "# You should never use in your workflow any quantity computed on the test data, \n",
    "# even for something as simple as data normalization.\n",
    "test_data_scaled = scaler.transform(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1.2 Building your network\n",
    "\n",
    "Because so few samples are available, you’ll use a very small network with two hidden layers, each with 64 units. **In general, the less training data you have, the worse overfitting will be**, and using a small network is one way to mitigate overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model definition\n",
    "\n",
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "def build_model():\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Dense(64, activation='relu',\n",
    "                           input_shape=(train_data_scaled.shape[1],)))\n",
    "    model.add(layers.Dense(64, activation='relu'))\n",
    "    \n",
    "    # The network ends with a single unit and no activation (it will be a linear layer). \n",
    "    # This is a typical setup for scalar regression (a regression where you’re trying \n",
    "    # to predict a single continuous value).\n",
    "    model.add(layers.Dense(1))\n",
    "    \n",
    "    # compile the network with the mse loss function—mean squared error,\n",
    "    # the square of the difference between the predictions and the targets. \n",
    "    # This is a widely used loss function for regression problems.\n",
    "    model.compile(optimizer='rmsprop', loss='mse', metrics=['mae'])\n",
    "    \n",
    "    # You’re also monitoring a new metric during training: mean absolute error (MAE). \n",
    "    # It’s the absolute value of the difference between the predictions and the targets.\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1.3 Validating your approach using K-fold validation\n",
    "\n",
    "- To evaluate your network while you keep adjusting its parameters (such as the number of epochs used for training), you could **split the data into a training set and a validation set**\n",
    "- Because you have so **few data points**, the validation set would end up being very small.\n",
    "- The best practice in such situations is to use **K-fold cross-validation**.\n",
    "    - It consists of splitting the available data into K partitions (typically K = 4 or 5)\n",
    "    - Instantiating K identical models, and training each one on K – 1 partitions while evaluating on the remaining partition. \n",
    "    - The **validation score** for the model used is then the **average of the K validation scores obtained**. \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 303 samples, validate on 101 samples\n",
      "Epoch 1/100\n",
      "303/303 [==============================] - 2s 7ms/step - loss: 205.3034 - mean_absolute_error: 10.9715 - val_loss: 30.4058 - val_mean_absolute_error: 3.6577\n",
      "Epoch 2/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 26.9031 - mean_absolute_error: 3.5319 - val_loss: 20.6360 - val_mean_absolute_error: 3.0265\n",
      "Epoch 3/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 20.2009 - mean_absolute_error: 3.0268 - val_loss: 16.3316 - val_mean_absolute_error: 2.8215\n",
      "Epoch 4/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 17.9474 - mean_absolute_error: 2.7880 - val_loss: 14.2374 - val_mean_absolute_error: 2.2671\n",
      "Epoch 5/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 16.1364 - mean_absolute_error: 2.6858 - val_loss: 14.0621 - val_mean_absolute_error: 2.1950\n",
      "Epoch 6/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 14.8039 - mean_absolute_error: 2.5254 - val_loss: 12.3838 - val_mean_absolute_error: 2.1377\n",
      "Epoch 7/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 13.8643 - mean_absolute_error: 2.4954 - val_loss: 11.0682 - val_mean_absolute_error: 1.9294\n",
      "Epoch 8/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 13.6188 - mean_absolute_error: 2.4004 - val_loss: 11.6391 - val_mean_absolute_error: 2.0415\n",
      "Epoch 9/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 13.2330 - mean_absolute_error: 2.4158 - val_loss: 10.3135 - val_mean_absolute_error: 2.1958\n",
      "Epoch 10/100\n",
      "303/303 [==============================] - 1s 4ms/step - loss: 12.6382 - mean_absolute_error: 2.3069 - val_loss: 11.5434 - val_mean_absolute_error: 2.0339\n",
      "Epoch 11/100\n",
      "303/303 [==============================] - 1s 4ms/step - loss: 12.7168 - mean_absolute_error: 2.2895 - val_loss: 9.7917 - val_mean_absolute_error: 2.3154\n",
      "Epoch 12/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 11.9917 - mean_absolute_error: 2.2046 - val_loss: 9.4844 - val_mean_absolute_error: 2.3075\n",
      "Epoch 13/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 11.3445 - mean_absolute_error: 2.2094 - val_loss: 8.6345 - val_mean_absolute_error: 1.9634\n",
      "Epoch 14/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 11.4159 - mean_absolute_error: 2.2063 - val_loss: 8.9582 - val_mean_absolute_error: 1.8389\n",
      "Epoch 15/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 11.4375 - mean_absolute_error: 2.1495 - val_loss: 8.5824 - val_mean_absolute_error: 1.8054\n",
      "Epoch 16/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 11.3020 - mean_absolute_error: 2.1870 - val_loss: 8.3126 - val_mean_absolute_error: 1.8060\n",
      "Epoch 17/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 10.4202 - mean_absolute_error: 2.1431 - val_loss: 8.2794 - val_mean_absolute_error: 2.1383\n",
      "Epoch 18/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 10.5674 - mean_absolute_error: 2.1579 - val_loss: 9.8821 - val_mean_absolute_error: 1.9420\n",
      "Epoch 19/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 10.2861 - mean_absolute_error: 2.1098 - val_loss: 8.6810 - val_mean_absolute_error: 1.8416\n",
      "Epoch 20/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 10.1278 - mean_absolute_error: 2.0257 - val_loss: 9.1700 - val_mean_absolute_error: 2.2359\n",
      "Epoch 21/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 10.3154 - mean_absolute_error: 2.0442 - val_loss: 7.5041 - val_mean_absolute_error: 1.9366\n",
      "Epoch 22/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 10.0301 - mean_absolute_error: 2.0706 - val_loss: 7.7630 - val_mean_absolute_error: 2.0090\n",
      "Epoch 23/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 9.5741 - mean_absolute_error: 1.9897 - val_loss: 7.2711 - val_mean_absolute_error: 1.7455\n",
      "Epoch 24/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 9.4488 - mean_absolute_error: 1.9898 - val_loss: 7.5307 - val_mean_absolute_error: 1.8089\n",
      "Epoch 25/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 9.4944 - mean_absolute_error: 1.9968 - val_loss: 8.0658 - val_mean_absolute_error: 1.8802\n",
      "Epoch 26/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 9.3027 - mean_absolute_error: 1.9887 - val_loss: 7.6331 - val_mean_absolute_error: 1.9009\n",
      "Epoch 27/100\n",
      "303/303 [==============================] - 1s 4ms/step - loss: 8.7160 - mean_absolute_error: 1.9288 - val_loss: 6.8500 - val_mean_absolute_error: 1.8819\n",
      "Epoch 28/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 8.8621 - mean_absolute_error: 1.9533 - val_loss: 8.7255 - val_mean_absolute_error: 1.8368\n",
      "Epoch 29/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 9.0566 - mean_absolute_error: 1.9509 - val_loss: 7.2149 - val_mean_absolute_error: 1.8997\n",
      "Epoch 30/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 8.5796 - mean_absolute_error: 1.9657 - val_loss: 8.9580 - val_mean_absolute_error: 2.3323\n",
      "Epoch 31/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 8.6656 - mean_absolute_error: 1.8857 - val_loss: 7.7347 - val_mean_absolute_error: 2.0365\n",
      "Epoch 32/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 8.9700 - mean_absolute_error: 1.9435 - val_loss: 7.1988 - val_mean_absolute_error: 1.9690\n",
      "Epoch 33/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 8.2088 - mean_absolute_error: 1.8461 - val_loss: 7.5269 - val_mean_absolute_error: 1.8123\n",
      "Epoch 34/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 8.1419 - mean_absolute_error: 1.8108 - val_loss: 8.1887 - val_mean_absolute_error: 2.2207\n",
      "Epoch 35/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 8.2485 - mean_absolute_error: 1.9057 - val_loss: 7.4418 - val_mean_absolute_error: 1.9233\n",
      "Epoch 36/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 8.2168 - mean_absolute_error: 1.8782 - val_loss: 7.9760 - val_mean_absolute_error: 1.9945\n",
      "Epoch 37/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 7.6810 - mean_absolute_error: 1.8075 - val_loss: 10.3950 - val_mean_absolute_error: 2.3291\n",
      "Epoch 38/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 8.1793 - mean_absolute_error: 1.8142 - val_loss: 6.7144 - val_mean_absolute_error: 1.8486\n",
      "Epoch 39/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 7.8647 - mean_absolute_error: 1.8199 - val_loss: 12.6848 - val_mean_absolute_error: 2.8388\n",
      "Epoch 40/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 8.2623 - mean_absolute_error: 1.8103 - val_loss: 7.6335 - val_mean_absolute_error: 1.9524\n",
      "Epoch 41/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 8.1108 - mean_absolute_error: 1.8170 - val_loss: 8.9983 - val_mean_absolute_error: 2.2777\n",
      "Epoch 42/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 7.6021 - mean_absolute_error: 1.8814 - val_loss: 7.0724 - val_mean_absolute_error: 1.8463\n",
      "Epoch 43/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 7.7090 - mean_absolute_error: 1.7986 - val_loss: 7.3174 - val_mean_absolute_error: 1.7969\n",
      "Epoch 44/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 7.4400 - mean_absolute_error: 1.7361 - val_loss: 7.5207 - val_mean_absolute_error: 1.8038\n",
      "Epoch 45/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 7.3531 - mean_absolute_error: 1.8047 - val_loss: 7.8430 - val_mean_absolute_error: 2.0334\n",
      "Epoch 46/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 7.0333 - mean_absolute_error: 1.7565 - val_loss: 7.5094 - val_mean_absolute_error: 1.9303\n",
      "Epoch 47/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 7.3999 - mean_absolute_error: 1.7673 - val_loss: 7.9015 - val_mean_absolute_error: 1.8663\n",
      "Epoch 48/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 7.2563 - mean_absolute_error: 1.7113 - val_loss: 6.5673 - val_mean_absolute_error: 1.8101\n",
      "Epoch 49/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 7.1863 - mean_absolute_error: 1.7205 - val_loss: 7.3799 - val_mean_absolute_error: 1.9881\n",
      "Epoch 50/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 7.1705 - mean_absolute_error: 1.6774 - val_loss: 7.9701 - val_mean_absolute_error: 1.8696\n",
      "Epoch 51/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 7.0563 - mean_absolute_error: 1.7003 - val_loss: 6.7705 - val_mean_absolute_error: 1.7475\n",
      "Epoch 52/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 7.0619 - mean_absolute_error: 1.7405 - val_loss: 8.5012 - val_mean_absolute_error: 2.1350\n",
      "Epoch 53/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 6.4851 - mean_absolute_error: 1.7230 - val_loss: 8.2862 - val_mean_absolute_error: 2.1327\n",
      "Epoch 54/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 6.7760 - mean_absolute_error: 1.7045 - val_loss: 7.5807 - val_mean_absolute_error: 1.9687\n",
      "Epoch 55/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 6.2020 - mean_absolute_error: 1.6329 - val_loss: 11.2125 - val_mean_absolute_error: 2.6348\n",
      "Epoch 56/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 6.7943 - mean_absolute_error: 1.7488 - val_loss: 7.5355 - val_mean_absolute_error: 1.9848\n",
      "Epoch 57/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 6.9756 - mean_absolute_error: 1.6768 - val_loss: 6.5118 - val_mean_absolute_error: 1.6985\n",
      "Epoch 58/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 6.6312 - mean_absolute_error: 1.6505 - val_loss: 8.1461 - val_mean_absolute_error: 2.0360\n",
      "Epoch 59/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 6.6545 - mean_absolute_error: 1.6315 - val_loss: 7.5160 - val_mean_absolute_error: 1.9664\n",
      "Epoch 60/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 6.8734 - mean_absolute_error: 1.6784 - val_loss: 7.4922 - val_mean_absolute_error: 1.9658\n",
      "Epoch 61/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 6.6659 - mean_absolute_error: 1.6478 - val_loss: 7.3378 - val_mean_absolute_error: 1.8587\n",
      "Epoch 62/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 6.2385 - mean_absolute_error: 1.6393 - val_loss: 7.1055 - val_mean_absolute_error: 1.8519\n",
      "Epoch 63/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 6.1056 - mean_absolute_error: 1.5878 - val_loss: 6.5586 - val_mean_absolute_error: 1.8186\n",
      "Epoch 64/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 6.0976 - mean_absolute_error: 1.6205 - val_loss: 7.3778 - val_mean_absolute_error: 1.9282\n",
      "Epoch 65/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 6.3840 - mean_absolute_error: 1.6826 - val_loss: 7.8888 - val_mean_absolute_error: 2.0935\n",
      "Epoch 66/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 6.6793 - mean_absolute_error: 1.5870 - val_loss: 8.2349 - val_mean_absolute_error: 2.0771\n",
      "Epoch 67/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 5.9384 - mean_absolute_error: 1.5606 - val_loss: 9.4582 - val_mean_absolute_error: 2.2415\n",
      "Epoch 68/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 6.2789 - mean_absolute_error: 1.5972 - val_loss: 7.6366 - val_mean_absolute_error: 1.8140\n",
      "Epoch 69/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 6.1042 - mean_absolute_error: 1.6365 - val_loss: 7.8064 - val_mean_absolute_error: 2.0690\n",
      "Epoch 70/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 5.8177 - mean_absolute_error: 1.5684 - val_loss: 6.8281 - val_mean_absolute_error: 1.7824\n",
      "Epoch 71/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 6.0622 - mean_absolute_error: 1.5727 - val_loss: 7.3964 - val_mean_absolute_error: 2.0261\n",
      "Epoch 72/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 5.9040 - mean_absolute_error: 1.5274 - val_loss: 7.5202 - val_mean_absolute_error: 2.0083\n",
      "Epoch 73/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 5.5566 - mean_absolute_error: 1.5155 - val_loss: 7.0681 - val_mean_absolute_error: 1.9075\n",
      "Epoch 74/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 5.6106 - mean_absolute_error: 1.4663 - val_loss: 10.2978 - val_mean_absolute_error: 2.5521\n",
      "Epoch 75/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 4.8082 - mean_absolute_error: 1.4713 - val_loss: 7.8295 - val_mean_absolute_error: 1.8494\n",
      "Epoch 76/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 5.6912 - mean_absolute_error: 1.5940 - val_loss: 8.0088 - val_mean_absolute_error: 2.0493\n",
      "Epoch 77/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 5.9523 - mean_absolute_error: 1.5118 - val_loss: 8.1552 - val_mean_absolute_error: 2.1332\n",
      "Epoch 78/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 5.7782 - mean_absolute_error: 1.5060 - val_loss: 7.5061 - val_mean_absolute_error: 1.9119\n",
      "Epoch 79/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 5.5380 - mean_absolute_error: 1.4674 - val_loss: 7.8684 - val_mean_absolute_error: 2.0167\n",
      "Epoch 80/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 5.3850 - mean_absolute_error: 1.4868 - val_loss: 7.9819 - val_mean_absolute_error: 2.0893\n",
      "Epoch 81/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 5.5247 - mean_absolute_error: 1.4436 - val_loss: 7.8678 - val_mean_absolute_error: 2.1137\n",
      "Epoch 82/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 5.4474 - mean_absolute_error: 1.5047 - val_loss: 7.9272 - val_mean_absolute_error: 2.2310\n",
      "Epoch 83/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 5.0291 - mean_absolute_error: 1.4398 - val_loss: 8.6494 - val_mean_absolute_error: 2.1034\n",
      "Epoch 84/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 5.1002 - mean_absolute_error: 1.4269 - val_loss: 8.7385 - val_mean_absolute_error: 2.1034\n",
      "Epoch 85/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 5.1221 - mean_absolute_error: 1.4612 - val_loss: 10.6902 - val_mean_absolute_error: 2.5179\n",
      "Epoch 86/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 5.4426 - mean_absolute_error: 1.4813 - val_loss: 8.1329 - val_mean_absolute_error: 2.1266\n",
      "Epoch 87/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 5.1117 - mean_absolute_error: 1.4304 - val_loss: 8.4776 - val_mean_absolute_error: 2.1409\n",
      "Epoch 88/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 5.1398 - mean_absolute_error: 1.4591 - val_loss: 7.0558 - val_mean_absolute_error: 1.8617\n",
      "Epoch 89/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 5.3213 - mean_absolute_error: 1.4955 - val_loss: 6.8416 - val_mean_absolute_error: 1.8012\n",
      "Epoch 90/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 4.9494 - mean_absolute_error: 1.4027 - val_loss: 8.6074 - val_mean_absolute_error: 2.2025\n",
      "Epoch 91/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 4.6377 - mean_absolute_error: 1.3847 - val_loss: 8.3668 - val_mean_absolute_error: 1.9260\n",
      "Epoch 92/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 5.2606 - mean_absolute_error: 1.3951 - val_loss: 8.0419 - val_mean_absolute_error: 1.8981\n",
      "Epoch 93/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 4.9999 - mean_absolute_error: 1.4432 - val_loss: 7.2544 - val_mean_absolute_error: 1.9748\n",
      "Epoch 94/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 4.7062 - mean_absolute_error: 1.4139 - val_loss: 8.8574 - val_mean_absolute_error: 2.1193\n",
      "Epoch 95/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 5.1025 - mean_absolute_error: 1.3852 - val_loss: 7.8948 - val_mean_absolute_error: 2.0746\n",
      "Epoch 96/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 4.5566 - mean_absolute_error: 1.4032 - val_loss: 10.5611 - val_mean_absolute_error: 2.4378\n",
      "Epoch 97/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 4.9218 - mean_absolute_error: 1.4009 - val_loss: 10.0741 - val_mean_absolute_error: 2.2866\n",
      "Epoch 98/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 5.2103 - mean_absolute_error: 1.4202 - val_loss: 8.4349 - val_mean_absolute_error: 2.2043\n",
      "Epoch 99/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 4.3255 - mean_absolute_error: 1.4041 - val_loss: 11.0119 - val_mean_absolute_error: 2.3566\n",
      "Epoch 100/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 4.6502 - mean_absolute_error: 1.3784 - val_loss: 8.5548 - val_mean_absolute_error: 2.1640\n",
      "Train on 303 samples, validate on 101 samples\n",
      "Epoch 1/100\n",
      "303/303 [==============================] - 2s 6ms/step - loss: 223.1781 - mean_absolute_error: 11.3318 - val_loss: 38.4835 - val_mean_absolute_error: 4.6771\n",
      "Epoch 2/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 32.7659 - mean_absolute_error: 3.7669 - val_loss: 16.8632 - val_mean_absolute_error: 3.0893\n",
      "Epoch 3/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 20.2922 - mean_absolute_error: 2.9220 - val_loss: 15.4126 - val_mean_absolute_error: 2.9661\n",
      "Epoch 4/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 17.5476 - mean_absolute_error: 2.6958 - val_loss: 14.5978 - val_mean_absolute_error: 3.0556\n",
      "Epoch 5/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 15.3614 - mean_absolute_error: 2.5691 - val_loss: 13.5174 - val_mean_absolute_error: 2.7913\n",
      "Epoch 6/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 14.0345 - mean_absolute_error: 2.4954 - val_loss: 12.8212 - val_mean_absolute_error: 2.7274\n",
      "Epoch 7/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 13.4162 - mean_absolute_error: 2.3847 - val_loss: 11.8164 - val_mean_absolute_error: 2.5912\n",
      "Epoch 8/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 12.6551 - mean_absolute_error: 2.2976 - val_loss: 11.1247 - val_mean_absolute_error: 2.5536\n",
      "Epoch 9/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 11.6980 - mean_absolute_error: 2.2000 - val_loss: 13.7347 - val_mean_absolute_error: 2.9610\n",
      "Epoch 10/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 11.4878 - mean_absolute_error: 2.2139 - val_loss: 12.3648 - val_mean_absolute_error: 2.7839\n",
      "Epoch 11/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 11.0060 - mean_absolute_error: 2.1766 - val_loss: 11.8335 - val_mean_absolute_error: 2.7093\n",
      "Epoch 12/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 10.3613 - mean_absolute_error: 2.1323 - val_loss: 11.8312 - val_mean_absolute_error: 2.7259\n",
      "Epoch 13/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 10.4087 - mean_absolute_error: 2.1211 - val_loss: 10.8963 - val_mean_absolute_error: 2.4996\n",
      "Epoch 14/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 10.5138 - mean_absolute_error: 2.0546 - val_loss: 10.0819 - val_mean_absolute_error: 2.4416\n",
      "Epoch 15/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 9.7006 - mean_absolute_error: 2.0655 - val_loss: 12.1060 - val_mean_absolute_error: 2.6394\n",
      "Epoch 16/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 10.1820 - mean_absolute_error: 2.0457 - val_loss: 11.1716 - val_mean_absolute_error: 2.6485\n",
      "Epoch 17/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 9.5144 - mean_absolute_error: 2.0911 - val_loss: 11.4346 - val_mean_absolute_error: 2.6973\n",
      "Epoch 18/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 9.9572 - mean_absolute_error: 2.0431 - val_loss: 10.3093 - val_mean_absolute_error: 2.5492\n",
      "Epoch 19/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 9.1633 - mean_absolute_error: 2.0119 - val_loss: 11.3563 - val_mean_absolute_error: 2.6576\n",
      "Epoch 20/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 9.3321 - mean_absolute_error: 2.0288 - val_loss: 9.6431 - val_mean_absolute_error: 2.4185\n",
      "Epoch 21/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 8.6637 - mean_absolute_error: 1.9226 - val_loss: 12.1243 - val_mean_absolute_error: 2.7524\n",
      "Epoch 22/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 9.2885 - mean_absolute_error: 1.9868 - val_loss: 12.3197 - val_mean_absolute_error: 2.7322\n",
      "Epoch 23/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 8.5105 - mean_absolute_error: 1.9155 - val_loss: 11.0058 - val_mean_absolute_error: 2.5837\n",
      "Epoch 24/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 9.0878 - mean_absolute_error: 1.9363 - val_loss: 10.5130 - val_mean_absolute_error: 2.5467\n",
      "Epoch 25/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 8.7310 - mean_absolute_error: 1.9320 - val_loss: 9.2992 - val_mean_absolute_error: 2.3582\n",
      "Epoch 26/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 8.7983 - mean_absolute_error: 1.9223 - val_loss: 10.2124 - val_mean_absolute_error: 2.5288\n",
      "Epoch 27/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 8.3715 - mean_absolute_error: 1.9635 - val_loss: 10.1439 - val_mean_absolute_error: 2.5428\n",
      "Epoch 28/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 7.8437 - mean_absolute_error: 1.8228 - val_loss: 10.7162 - val_mean_absolute_error: 2.6150\n",
      "Epoch 29/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 8.5684 - mean_absolute_error: 1.8957 - val_loss: 11.0430 - val_mean_absolute_error: 2.6261\n",
      "Epoch 30/100\n",
      "303/303 [==============================] - 1s 4ms/step - loss: 7.7117 - mean_absolute_error: 1.7571 - val_loss: 9.1589 - val_mean_absolute_error: 2.3907\n",
      "Epoch 31/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 8.2169 - mean_absolute_error: 1.8416 - val_loss: 9.1556 - val_mean_absolute_error: 2.3677\n",
      "Epoch 32/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 7.8735 - mean_absolute_error: 1.7846 - val_loss: 11.4970 - val_mean_absolute_error: 2.7060\n",
      "Epoch 33/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 8.0491 - mean_absolute_error: 1.8480 - val_loss: 9.5621 - val_mean_absolute_error: 2.4394\n",
      "Epoch 34/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 7.7216 - mean_absolute_error: 1.8206 - val_loss: 10.2899 - val_mean_absolute_error: 2.5498\n",
      "Epoch 35/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 7.4400 - mean_absolute_error: 1.8000 - val_loss: 9.3210 - val_mean_absolute_error: 2.3787\n",
      "Epoch 36/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 7.2773 - mean_absolute_error: 1.7866 - val_loss: 9.9700 - val_mean_absolute_error: 2.5019\n",
      "Epoch 37/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 7.3959 - mean_absolute_error: 1.7864 - val_loss: 12.1199 - val_mean_absolute_error: 2.7931\n",
      "Epoch 38/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 7.5560 - mean_absolute_error: 1.7854 - val_loss: 9.1051 - val_mean_absolute_error: 2.3373\n",
      "Epoch 39/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 7.0431 - mean_absolute_error: 1.7088 - val_loss: 10.5701 - val_mean_absolute_error: 2.5611\n",
      "Epoch 40/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 6.8460 - mean_absolute_error: 1.7854 - val_loss: 10.9908 - val_mean_absolute_error: 2.6247\n",
      "Epoch 41/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 6.7574 - mean_absolute_error: 1.7124 - val_loss: 9.0624 - val_mean_absolute_error: 2.3438\n",
      "Epoch 42/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 7.1919 - mean_absolute_error: 1.7879 - val_loss: 10.6613 - val_mean_absolute_error: 2.6409\n",
      "Epoch 43/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 6.9395 - mean_absolute_error: 1.7089 - val_loss: 9.3017 - val_mean_absolute_error: 2.3753\n",
      "Epoch 44/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 6.9296 - mean_absolute_error: 1.7240 - val_loss: 10.2248 - val_mean_absolute_error: 2.4896\n",
      "Epoch 45/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 6.4451 - mean_absolute_error: 1.6194 - val_loss: 10.6328 - val_mean_absolute_error: 2.5582\n",
      "Epoch 46/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 6.1160 - mean_absolute_error: 1.6940 - val_loss: 11.5949 - val_mean_absolute_error: 2.6260\n",
      "Epoch 47/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 6.5191 - mean_absolute_error: 1.6946 - val_loss: 9.5530 - val_mean_absolute_error: 2.3531\n",
      "Epoch 48/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 6.1390 - mean_absolute_error: 1.6656 - val_loss: 8.9609 - val_mean_absolute_error: 2.3575\n",
      "Epoch 49/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 6.2458 - mean_absolute_error: 1.6681 - val_loss: 8.8515 - val_mean_absolute_error: 2.3289\n",
      "Epoch 50/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 5.9390 - mean_absolute_error: 1.5806 - val_loss: 11.6045 - val_mean_absolute_error: 2.6732\n",
      "Epoch 51/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 6.1566 - mean_absolute_error: 1.6197 - val_loss: 8.7201 - val_mean_absolute_error: 2.2987\n",
      "Epoch 52/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 6.0341 - mean_absolute_error: 1.5547 - val_loss: 12.8412 - val_mean_absolute_error: 2.7808\n",
      "Epoch 53/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 6.0146 - mean_absolute_error: 1.6024 - val_loss: 8.2788 - val_mean_absolute_error: 2.2180\n",
      "Epoch 54/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 5.7438 - mean_absolute_error: 1.6150 - val_loss: 8.4680 - val_mean_absolute_error: 2.2940\n",
      "Epoch 55/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 5.6024 - mean_absolute_error: 1.5304 - val_loss: 9.4567 - val_mean_absolute_error: 2.3685\n",
      "Epoch 56/100\n",
      "303/303 [==============================] - 1s 4ms/step - loss: 6.0245 - mean_absolute_error: 1.5738 - val_loss: 8.1615 - val_mean_absolute_error: 2.1720\n",
      "Epoch 57/100\n",
      "303/303 [==============================] - 1s 4ms/step - loss: 5.6372 - mean_absolute_error: 1.5187 - val_loss: 10.4229 - val_mean_absolute_error: 2.5507\n",
      "Epoch 58/100\n",
      "303/303 [==============================] - 1s 4ms/step - loss: 5.2997 - mean_absolute_error: 1.4881 - val_loss: 10.3419 - val_mean_absolute_error: 2.4400\n",
      "Epoch 59/100\n",
      "303/303 [==============================] - 1s 4ms/step - loss: 5.4308 - mean_absolute_error: 1.4799 - val_loss: 9.0543 - val_mean_absolute_error: 2.3000\n",
      "Epoch 60/100\n",
      "303/303 [==============================] - 1s 4ms/step - loss: 5.1361 - mean_absolute_error: 1.5289 - val_loss: 8.8123 - val_mean_absolute_error: 2.2593\n",
      "Epoch 61/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 5.0277 - mean_absolute_error: 1.4994 - val_loss: 10.6407 - val_mean_absolute_error: 2.5855\n",
      "Epoch 62/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 5.1395 - mean_absolute_error: 1.5349 - val_loss: 9.6087 - val_mean_absolute_error: 2.3670\n",
      "Epoch 63/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 4.6535 - mean_absolute_error: 1.4550 - val_loss: 18.9180 - val_mean_absolute_error: 3.3999\n",
      "Epoch 64/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 5.3760 - mean_absolute_error: 1.4956 - val_loss: 9.8226 - val_mean_absolute_error: 2.3584\n",
      "Epoch 65/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 5.1050 - mean_absolute_error: 1.4622 - val_loss: 9.1333 - val_mean_absolute_error: 2.3285\n",
      "Epoch 66/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 5.0276 - mean_absolute_error: 1.4665 - val_loss: 9.6955 - val_mean_absolute_error: 2.3902\n",
      "Epoch 67/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 5.0002 - mean_absolute_error: 1.4940 - val_loss: 10.9845 - val_mean_absolute_error: 2.5356\n",
      "Epoch 68/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 4.8301 - mean_absolute_error: 1.4550 - val_loss: 10.2442 - val_mean_absolute_error: 2.4603\n",
      "Epoch 69/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 4.7341 - mean_absolute_error: 1.4039 - val_loss: 9.1483 - val_mean_absolute_error: 2.3550\n",
      "Epoch 70/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 4.7725 - mean_absolute_error: 1.4250 - val_loss: 9.8086 - val_mean_absolute_error: 2.3827\n",
      "Epoch 71/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 4.4816 - mean_absolute_error: 1.3525 - val_loss: 10.5043 - val_mean_absolute_error: 2.5657\n",
      "Epoch 72/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 4.8282 - mean_absolute_error: 1.3765 - val_loss: 8.6628 - val_mean_absolute_error: 2.2362\n",
      "Epoch 73/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 4.1827 - mean_absolute_error: 1.3877 - val_loss: 10.1404 - val_mean_absolute_error: 2.5050\n",
      "Epoch 74/100\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 4.3381 - mean_absolute_error: 1.3691 - val_loss: 8.9149 - val_mean_absolute_error: 2.2664\n",
      "Epoch 75/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 4.1886 - mean_absolute_error: 1.3674 - val_loss: 9.8097 - val_mean_absolute_error: 2.5007\n",
      "Epoch 76/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 4.2455 - mean_absolute_error: 1.3352 - val_loss: 8.2415 - val_mean_absolute_error: 2.1950\n",
      "Epoch 77/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 4.4439 - mean_absolute_error: 1.3854 - val_loss: 9.9122 - val_mean_absolute_error: 2.3416\n",
      "Epoch 78/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 4.1206 - mean_absolute_error: 1.3267 - val_loss: 8.6739 - val_mean_absolute_error: 2.1351\n",
      "Epoch 79/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 4.0520 - mean_absolute_error: 1.3396 - val_loss: 10.0751 - val_mean_absolute_error: 2.4787\n",
      "Epoch 80/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 4.0318 - mean_absolute_error: 1.3137 - val_loss: 9.0052 - val_mean_absolute_error: 2.2945\n",
      "Epoch 81/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 4.2199 - mean_absolute_error: 1.3351 - val_loss: 11.5070 - val_mean_absolute_error: 2.5641\n",
      "Epoch 82/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 3.8460 - mean_absolute_error: 1.3133 - val_loss: 9.8066 - val_mean_absolute_error: 2.4178\n",
      "Epoch 83/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 3.6724 - mean_absolute_error: 1.2580 - val_loss: 10.8957 - val_mean_absolute_error: 2.5654\n",
      "Epoch 84/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 3.8452 - mean_absolute_error: 1.2730 - val_loss: 13.2639 - val_mean_absolute_error: 2.7791\n",
      "Epoch 85/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 3.5935 - mean_absolute_error: 1.2928 - val_loss: 12.7594 - val_mean_absolute_error: 2.7435\n",
      "Epoch 86/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 3.5892 - mean_absolute_error: 1.2347 - val_loss: 11.1937 - val_mean_absolute_error: 2.5001\n",
      "Epoch 87/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 3.3827 - mean_absolute_error: 1.2779 - val_loss: 10.5136 - val_mean_absolute_error: 2.4612\n",
      "Epoch 88/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 3.3065 - mean_absolute_error: 1.2332 - val_loss: 9.3870 - val_mean_absolute_error: 2.2534\n",
      "Epoch 89/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 3.5095 - mean_absolute_error: 1.2536 - val_loss: 10.9549 - val_mean_absolute_error: 2.5148\n",
      "Epoch 90/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 3.5944 - mean_absolute_error: 1.2360 - val_loss: 10.6448 - val_mean_absolute_error: 2.5511\n",
      "Epoch 91/100\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 3.3752 - mean_absolute_error: 1.2212 - val_loss: 11.0654 - val_mean_absolute_error: 2.4234\n",
      "Epoch 92/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 3.3652 - mean_absolute_error: 1.2208 - val_loss: 11.1305 - val_mean_absolute_error: 2.4850\n",
      "Epoch 93/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 3.3398 - mean_absolute_error: 1.2322 - val_loss: 13.3754 - val_mean_absolute_error: 2.5412\n",
      "Epoch 94/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 3.1247 - mean_absolute_error: 1.2421 - val_loss: 8.9953 - val_mean_absolute_error: 2.1822\n",
      "Epoch 95/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 3.4159 - mean_absolute_error: 1.2165 - val_loss: 10.6062 - val_mean_absolute_error: 2.2778\n",
      "Epoch 96/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 3.2585 - mean_absolute_error: 1.1971 - val_loss: 9.9965 - val_mean_absolute_error: 2.2852\n",
      "Epoch 97/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 3.0587 - mean_absolute_error: 1.1804 - val_loss: 11.8891 - val_mean_absolute_error: 2.5696\n",
      "Epoch 98/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 3.3502 - mean_absolute_error: 1.2175 - val_loss: 11.9308 - val_mean_absolute_error: 2.5730\n",
      "Epoch 99/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 3.1972 - mean_absolute_error: 1.1779 - val_loss: 11.5241 - val_mean_absolute_error: 2.3870\n",
      "Epoch 100/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 3.2550 - mean_absolute_error: 1.1949 - val_loss: 15.1842 - val_mean_absolute_error: 2.7540\n",
      "Train on 303 samples, validate on 101 samples\n",
      "Epoch 1/100\n",
      "303/303 [==============================] - 2s 6ms/step - loss: 202.9349 - mean_absolute_error: 10.5777 - val_loss: 30.3046 - val_mean_absolute_error: 3.7446\n",
      "Epoch 2/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 27.6612 - mean_absolute_error: 3.6574 - val_loss: 25.5827 - val_mean_absolute_error: 3.3375\n",
      "Epoch 3/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 20.8227 - mean_absolute_error: 3.0910 - val_loss: 20.8294 - val_mean_absolute_error: 2.8593\n",
      "Epoch 4/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 16.6881 - mean_absolute_error: 2.7362 - val_loss: 18.6842 - val_mean_absolute_error: 2.7320\n",
      "Epoch 5/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 15.0229 - mean_absolute_error: 2.5995 - val_loss: 17.3212 - val_mean_absolute_error: 2.6214\n",
      "Epoch 6/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 13.7192 - mean_absolute_error: 2.5227 - val_loss: 17.0349 - val_mean_absolute_error: 2.7028\n",
      "Epoch 7/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 11.6827 - mean_absolute_error: 2.3116 - val_loss: 18.2922 - val_mean_absolute_error: 3.0433\n",
      "Epoch 8/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 11.8169 - mean_absolute_error: 2.2836 - val_loss: 15.1842 - val_mean_absolute_error: 2.5471\n",
      "Epoch 9/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 11.4209 - mean_absolute_error: 2.2404 - val_loss: 15.5039 - val_mean_absolute_error: 2.5729\n",
      "Epoch 10/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 9.7184 - mean_absolute_error: 2.1496 - val_loss: 15.2696 - val_mean_absolute_error: 2.6687\n",
      "Epoch 11/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 10.3555 - mean_absolute_error: 2.1562 - val_loss: 14.7181 - val_mean_absolute_error: 2.6886\n",
      "Epoch 12/100\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 9.3283 - mean_absolute_error: 2.1341 - val_loss: 14.9292 - val_mean_absolute_error: 2.6553\n",
      "Epoch 13/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 9.2649 - mean_absolute_error: 2.0711 - val_loss: 15.9235 - val_mean_absolute_error: 2.7314\n",
      "Epoch 14/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 8.9613 - mean_absolute_error: 2.0726 - val_loss: 14.9512 - val_mean_absolute_error: 2.5640\n",
      "Epoch 15/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 9.3390 - mean_absolute_error: 2.0482 - val_loss: 14.9871 - val_mean_absolute_error: 2.6152\n",
      "Epoch 16/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 9.3070 - mean_absolute_error: 2.0146 - val_loss: 14.9746 - val_mean_absolute_error: 2.5845\n",
      "Epoch 17/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 8.8756 - mean_absolute_error: 1.9837 - val_loss: 14.5105 - val_mean_absolute_error: 2.6964\n",
      "Epoch 18/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 8.2511 - mean_absolute_error: 1.9702 - val_loss: 15.0788 - val_mean_absolute_error: 2.6527\n",
      "Epoch 19/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 8.2977 - mean_absolute_error: 2.0340 - val_loss: 14.6540 - val_mean_absolute_error: 2.5276\n",
      "Epoch 20/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 8.2609 - mean_absolute_error: 1.9406 - val_loss: 15.7160 - val_mean_absolute_error: 2.6791\n",
      "Epoch 21/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 7.9194 - mean_absolute_error: 1.9033 - val_loss: 15.1665 - val_mean_absolute_error: 2.5915\n",
      "Epoch 22/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 8.0268 - mean_absolute_error: 1.9922 - val_loss: 15.8758 - val_mean_absolute_error: 2.7011\n",
      "Epoch 23/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 7.6278 - mean_absolute_error: 1.8772 - val_loss: 16.9787 - val_mean_absolute_error: 2.9575\n",
      "Epoch 24/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 7.5719 - mean_absolute_error: 1.9112 - val_loss: 15.4291 - val_mean_absolute_error: 2.7241\n",
      "Epoch 25/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 7.2097 - mean_absolute_error: 1.8273 - val_loss: 16.0168 - val_mean_absolute_error: 2.8244\n",
      "Epoch 26/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 7.1040 - mean_absolute_error: 1.8395 - val_loss: 14.7368 - val_mean_absolute_error: 2.5291\n",
      "Epoch 27/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 7.6451 - mean_absolute_error: 1.8957 - val_loss: 14.3630 - val_mean_absolute_error: 2.5195\n",
      "Epoch 28/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 6.8293 - mean_absolute_error: 1.8138 - val_loss: 14.3085 - val_mean_absolute_error: 2.4835\n",
      "Epoch 29/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 7.3149 - mean_absolute_error: 1.8144 - val_loss: 15.9302 - val_mean_absolute_error: 2.6365\n",
      "Epoch 30/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 7.0167 - mean_absolute_error: 1.8021 - val_loss: 15.0717 - val_mean_absolute_error: 2.6686\n",
      "Epoch 31/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 6.6738 - mean_absolute_error: 1.7814 - val_loss: 14.8551 - val_mean_absolute_error: 2.4656\n",
      "Epoch 32/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 6.7579 - mean_absolute_error: 1.7666 - val_loss: 19.4657 - val_mean_absolute_error: 2.8786\n",
      "Epoch 33/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 6.6947 - mean_absolute_error: 1.7374 - val_loss: 15.7407 - val_mean_absolute_error: 2.5713\n",
      "Epoch 34/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 6.4265 - mean_absolute_error: 1.7017 - val_loss: 15.1095 - val_mean_absolute_error: 2.6363\n",
      "Epoch 35/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 6.3774 - mean_absolute_error: 1.7341 - val_loss: 16.3066 - val_mean_absolute_error: 2.9265\n",
      "Epoch 36/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 6.4464 - mean_absolute_error: 1.7215 - val_loss: 15.1674 - val_mean_absolute_error: 2.6065\n",
      "Epoch 37/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 6.1594 - mean_absolute_error: 1.7052 - val_loss: 16.2328 - val_mean_absolute_error: 2.8529\n",
      "Epoch 38/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 6.2986 - mean_absolute_error: 1.6937 - val_loss: 16.8686 - val_mean_absolute_error: 2.8794\n",
      "Epoch 39/100\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 6.3418 - mean_absolute_error: 1.7252 - val_loss: 16.9593 - val_mean_absolute_error: 2.9252\n",
      "Epoch 40/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 5.9365 - mean_absolute_error: 1.7010 - val_loss: 15.7505 - val_mean_absolute_error: 2.6769\n",
      "Epoch 41/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 5.9610 - mean_absolute_error: 1.6871 - val_loss: 17.3573 - val_mean_absolute_error: 2.6707\n",
      "Epoch 42/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 6.1027 - mean_absolute_error: 1.6649 - val_loss: 14.5115 - val_mean_absolute_error: 2.5028\n",
      "Epoch 43/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 5.7391 - mean_absolute_error: 1.6449 - val_loss: 15.8302 - val_mean_absolute_error: 2.5979\n",
      "Epoch 44/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 5.7848 - mean_absolute_error: 1.6282 - val_loss: 18.6368 - val_mean_absolute_error: 2.7907\n",
      "Epoch 45/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 5.7450 - mean_absolute_error: 1.6693 - val_loss: 16.1741 - val_mean_absolute_error: 2.7007\n",
      "Epoch 46/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 5.7241 - mean_absolute_error: 1.5923 - val_loss: 16.1890 - val_mean_absolute_error: 2.6327\n",
      "Epoch 47/100\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 5.5534 - mean_absolute_error: 1.6097 - val_loss: 16.0153 - val_mean_absolute_error: 2.7579\n",
      "Epoch 48/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 5.6556 - mean_absolute_error: 1.6266 - val_loss: 16.6703 - val_mean_absolute_error: 2.8209\n",
      "Epoch 49/100\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 5.5176 - mean_absolute_error: 1.5731 - val_loss: 16.0444 - val_mean_absolute_error: 2.7314\n",
      "Epoch 50/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 5.2177 - mean_absolute_error: 1.5500 - val_loss: 16.4702 - val_mean_absolute_error: 2.6850\n",
      "Epoch 51/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 5.1401 - mean_absolute_error: 1.5623 - val_loss: 15.8217 - val_mean_absolute_error: 2.7072\n",
      "Epoch 52/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 4.8361 - mean_absolute_error: 1.5330 - val_loss: 17.2457 - val_mean_absolute_error: 3.0017\n",
      "Epoch 53/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 5.0843 - mean_absolute_error: 1.5624 - val_loss: 14.8336 - val_mean_absolute_error: 2.5657\n",
      "Epoch 54/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 4.7069 - mean_absolute_error: 1.5110 - val_loss: 16.3293 - val_mean_absolute_error: 2.5814\n",
      "Epoch 55/100\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 5.0636 - mean_absolute_error: 1.5750 - val_loss: 16.0460 - val_mean_absolute_error: 2.6082\n",
      "Epoch 56/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 5.1381 - mean_absolute_error: 1.5536 - val_loss: 16.2549 - val_mean_absolute_error: 2.7543\n",
      "Epoch 57/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 4.7327 - mean_absolute_error: 1.5206 - val_loss: 16.2365 - val_mean_absolute_error: 2.5562\n",
      "Epoch 58/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 4.8192 - mean_absolute_error: 1.4800 - val_loss: 16.7190 - val_mean_absolute_error: 2.6942\n",
      "Epoch 59/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 4.7005 - mean_absolute_error: 1.5178 - val_loss: 16.2159 - val_mean_absolute_error: 2.7419\n",
      "Epoch 60/100\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 5.1330 - mean_absolute_error: 1.5471 - val_loss: 16.0706 - val_mean_absolute_error: 2.7892\n",
      "Epoch 61/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 4.4803 - mean_absolute_error: 1.5061 - val_loss: 15.7112 - val_mean_absolute_error: 2.7615\n",
      "Epoch 62/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 4.7445 - mean_absolute_error: 1.5070 - val_loss: 16.5654 - val_mean_absolute_error: 2.8649\n",
      "Epoch 63/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 4.1078 - mean_absolute_error: 1.4542 - val_loss: 15.8070 - val_mean_absolute_error: 2.5023\n",
      "Epoch 64/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 4.8318 - mean_absolute_error: 1.4776 - val_loss: 15.0857 - val_mean_absolute_error: 2.6999\n",
      "Epoch 65/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 4.0935 - mean_absolute_error: 1.4546 - val_loss: 15.9863 - val_mean_absolute_error: 2.6587\n",
      "Epoch 66/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 4.2405 - mean_absolute_error: 1.4347 - val_loss: 16.6463 - val_mean_absolute_error: 2.7001\n",
      "Epoch 67/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 4.3456 - mean_absolute_error: 1.4546 - val_loss: 15.2668 - val_mean_absolute_error: 2.5390\n",
      "Epoch 68/100\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 4.2193 - mean_absolute_error: 1.4430 - val_loss: 18.8316 - val_mean_absolute_error: 2.9550\n",
      "Epoch 69/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 4.1975 - mean_absolute_error: 1.4620 - val_loss: 17.3010 - val_mean_absolute_error: 2.7045\n",
      "Epoch 70/100\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 4.2134 - mean_absolute_error: 1.4752 - val_loss: 14.7633 - val_mean_absolute_error: 2.4489\n",
      "Epoch 71/100\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 4.2437 - mean_absolute_error: 1.4734 - val_loss: 16.1963 - val_mean_absolute_error: 2.6868\n",
      "Epoch 72/100\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 4.1155 - mean_absolute_error: 1.4465 - val_loss: 15.9601 - val_mean_absolute_error: 2.7265\n",
      "Epoch 73/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 4.2414 - mean_absolute_error: 1.4167 - val_loss: 16.3590 - val_mean_absolute_error: 2.7999\n",
      "Epoch 74/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 4.1859 - mean_absolute_error: 1.4215 - val_loss: 17.4687 - val_mean_absolute_error: 2.7808\n",
      "Epoch 75/100\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 4.1436 - mean_absolute_error: 1.3893 - val_loss: 16.0722 - val_mean_absolute_error: 2.6750\n",
      "Epoch 76/100\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 4.1233 - mean_absolute_error: 1.3911 - val_loss: 15.2592 - val_mean_absolute_error: 2.5214\n",
      "Epoch 77/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 3.9757 - mean_absolute_error: 1.3823 - val_loss: 16.7758 - val_mean_absolute_error: 2.6894\n",
      "Epoch 78/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 3.9541 - mean_absolute_error: 1.4096 - val_loss: 17.1952 - val_mean_absolute_error: 2.6514\n",
      "Epoch 79/100\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 4.0130 - mean_absolute_error: 1.3907 - val_loss: 16.7686 - val_mean_absolute_error: 2.7357\n",
      "Epoch 80/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 3.8137 - mean_absolute_error: 1.3714 - val_loss: 18.9026 - val_mean_absolute_error: 2.9734\n",
      "Epoch 81/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 4.0004 - mean_absolute_error: 1.3948 - val_loss: 17.5096 - val_mean_absolute_error: 2.8346\n",
      "Epoch 82/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 3.9713 - mean_absolute_error: 1.3670 - val_loss: 17.1204 - val_mean_absolute_error: 2.7230\n",
      "Epoch 83/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 3.8778 - mean_absolute_error: 1.3797 - val_loss: 17.6465 - val_mean_absolute_error: 2.9091\n",
      "Epoch 84/100\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 3.6478 - mean_absolute_error: 1.3683 - val_loss: 17.7459 - val_mean_absolute_error: 2.8648\n",
      "Epoch 85/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 3.6971 - mean_absolute_error: 1.3566 - val_loss: 17.2198 - val_mean_absolute_error: 2.7469\n",
      "Epoch 86/100\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 3.6565 - mean_absolute_error: 1.3544 - val_loss: 16.8298 - val_mean_absolute_error: 2.7634\n",
      "Epoch 87/100\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 3.7292 - mean_absolute_error: 1.3653 - val_loss: 16.8486 - val_mean_absolute_error: 2.7216\n",
      "Epoch 88/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 3.7945 - mean_absolute_error: 1.3487 - val_loss: 17.6988 - val_mean_absolute_error: 2.8723\n",
      "Epoch 89/100\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 3.4032 - mean_absolute_error: 1.3066 - val_loss: 17.4532 - val_mean_absolute_error: 2.8682\n",
      "Epoch 90/100\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 3.5980 - mean_absolute_error: 1.3524 - val_loss: 17.2839 - val_mean_absolute_error: 2.7215\n",
      "Epoch 91/100\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 3.3701 - mean_absolute_error: 1.2686 - val_loss: 17.5936 - val_mean_absolute_error: 2.8189\n",
      "Epoch 92/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 3.6426 - mean_absolute_error: 1.3748 - val_loss: 16.4951 - val_mean_absolute_error: 2.6719\n",
      "Epoch 93/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 3.4359 - mean_absolute_error: 1.2683 - val_loss: 17.5952 - val_mean_absolute_error: 2.8679\n",
      "Epoch 94/100\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 3.5339 - mean_absolute_error: 1.3357 - val_loss: 16.7936 - val_mean_absolute_error: 2.8005\n",
      "Epoch 95/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 3.3767 - mean_absolute_error: 1.2905 - val_loss: 18.1073 - val_mean_absolute_error: 2.7770\n",
      "Epoch 96/100\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 3.4227 - mean_absolute_error: 1.2936 - val_loss: 19.5068 - val_mean_absolute_error: 3.0885\n",
      "Epoch 97/100\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 3.2751 - mean_absolute_error: 1.2641 - val_loss: 16.2410 - val_mean_absolute_error: 2.6597\n",
      "Epoch 98/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 3.2273 - mean_absolute_error: 1.2918 - val_loss: 18.8868 - val_mean_absolute_error: 3.0104\n",
      "Epoch 99/100\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 3.1621 - mean_absolute_error: 1.3120 - val_loss: 18.3113 - val_mean_absolute_error: 2.8336\n",
      "Epoch 100/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 3.0714 - mean_absolute_error: 1.2495 - val_loss: 17.9770 - val_mean_absolute_error: 2.8730\n",
      "Train on 303 samples, validate on 101 samples\n",
      "Epoch 1/100\n",
      "303/303 [==============================] - 2s 6ms/step - loss: 161.1691 - mean_absolute_error: 9.2735 - val_loss: 59.0752 - val_mean_absolute_error: 5.5385\n",
      "Epoch 2/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 27.2357 - mean_absolute_error: 3.6824 - val_loss: 30.3151 - val_mean_absolute_error: 3.6036\n",
      "Epoch 3/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 19.3779 - mean_absolute_error: 2.8924 - val_loss: 29.2643 - val_mean_absolute_error: 3.7153\n",
      "Epoch 4/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 16.6059 - mean_absolute_error: 2.6232 - val_loss: 22.3984 - val_mean_absolute_error: 3.0228\n",
      "Epoch 5/100\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 14.5970 - mean_absolute_error: 2.4817 - val_loss: 21.8209 - val_mean_absolute_error: 3.0455\n",
      "Epoch 6/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 13.4977 - mean_absolute_error: 2.3982 - val_loss: 19.0881 - val_mean_absolute_error: 2.7607\n",
      "Epoch 7/100\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 12.3792 - mean_absolute_error: 2.3641 - val_loss: 17.6560 - val_mean_absolute_error: 2.6683\n",
      "Epoch 8/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 11.8714 - mean_absolute_error: 2.3203 - val_loss: 17.0964 - val_mean_absolute_error: 2.9423\n",
      "Epoch 9/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 11.6322 - mean_absolute_error: 2.2962 - val_loss: 15.6861 - val_mean_absolute_error: 2.6713\n",
      "Epoch 10/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 11.3310 - mean_absolute_error: 2.1917 - val_loss: 17.5642 - val_mean_absolute_error: 2.6619\n",
      "Epoch 11/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 10.8493 - mean_absolute_error: 2.1653 - val_loss: 15.1006 - val_mean_absolute_error: 2.5189\n",
      "Epoch 12/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 10.5985 - mean_absolute_error: 2.1551 - val_loss: 16.8399 - val_mean_absolute_error: 2.7091\n",
      "Epoch 13/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 10.8075 - mean_absolute_error: 2.1418 - val_loss: 15.2267 - val_mean_absolute_error: 2.7226\n",
      "Epoch 14/100\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 10.0941 - mean_absolute_error: 2.0755 - val_loss: 17.3294 - val_mean_absolute_error: 2.8500\n",
      "Epoch 15/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 10.2347 - mean_absolute_error: 2.1309 - val_loss: 14.5832 - val_mean_absolute_error: 2.5268\n",
      "Epoch 16/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 9.5561 - mean_absolute_error: 2.0501 - val_loss: 15.0029 - val_mean_absolute_error: 2.7198\n",
      "Epoch 17/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 9.2445 - mean_absolute_error: 2.0332 - val_loss: 15.7185 - val_mean_absolute_error: 2.8287\n",
      "Epoch 18/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 9.3911 - mean_absolute_error: 2.0234 - val_loss: 14.7657 - val_mean_absolute_error: 2.7060\n",
      "Epoch 19/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 9.2739 - mean_absolute_error: 2.0339 - val_loss: 14.2986 - val_mean_absolute_error: 2.4829\n",
      "Epoch 20/100\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 9.0953 - mean_absolute_error: 1.9864 - val_loss: 13.7999 - val_mean_absolute_error: 2.4906\n",
      "Epoch 21/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 9.2785 - mean_absolute_error: 2.0008 - val_loss: 17.7117 - val_mean_absolute_error: 2.8885\n",
      "Epoch 22/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 8.7400 - mean_absolute_error: 1.9652 - val_loss: 12.7667 - val_mean_absolute_error: 2.3832\n",
      "Epoch 23/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 8.6957 - mean_absolute_error: 1.8858 - val_loss: 13.2376 - val_mean_absolute_error: 2.4639\n",
      "Epoch 24/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 8.5315 - mean_absolute_error: 1.9455 - val_loss: 13.4446 - val_mean_absolute_error: 2.6039\n",
      "Epoch 25/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 8.1468 - mean_absolute_error: 1.8410 - val_loss: 13.4800 - val_mean_absolute_error: 2.6280\n",
      "Epoch 26/100\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 8.3627 - mean_absolute_error: 1.8932 - val_loss: 12.8622 - val_mean_absolute_error: 2.4714\n",
      "Epoch 27/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 8.4339 - mean_absolute_error: 1.8981 - val_loss: 12.4532 - val_mean_absolute_error: 2.4605\n",
      "Epoch 28/100\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 7.8839 - mean_absolute_error: 1.8747 - val_loss: 13.9510 - val_mean_absolute_error: 2.5594\n",
      "Epoch 29/100\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 8.1726 - mean_absolute_error: 1.8448 - val_loss: 13.3128 - val_mean_absolute_error: 2.6106\n",
      "Epoch 30/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 7.5688 - mean_absolute_error: 1.8214 - val_loss: 13.1596 - val_mean_absolute_error: 2.4309\n",
      "Epoch 31/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 7.8398 - mean_absolute_error: 1.8788 - val_loss: 13.1958 - val_mean_absolute_error: 2.4413\n",
      "Epoch 32/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 7.3216 - mean_absolute_error: 1.8271 - val_loss: 13.8369 - val_mean_absolute_error: 2.5271\n",
      "Epoch 33/100\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 7.6345 - mean_absolute_error: 1.8037 - val_loss: 13.1426 - val_mean_absolute_error: 2.5102\n",
      "Epoch 34/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 7.4194 - mean_absolute_error: 1.7957 - val_loss: 15.5774 - val_mean_absolute_error: 2.8026\n",
      "Epoch 35/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 7.2833 - mean_absolute_error: 1.7276 - val_loss: 12.8087 - val_mean_absolute_error: 2.5003\n",
      "Epoch 36/100\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 6.8809 - mean_absolute_error: 1.7395 - val_loss: 13.2107 - val_mean_absolute_error: 2.4406\n",
      "Epoch 37/100\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 7.3832 - mean_absolute_error: 1.7057 - val_loss: 14.2864 - val_mean_absolute_error: 2.6688\n",
      "Epoch 38/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 7.5686 - mean_absolute_error: 1.7532 - val_loss: 12.8674 - val_mean_absolute_error: 2.5189\n",
      "Epoch 39/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 6.4569 - mean_absolute_error: 1.7183 - val_loss: 13.6526 - val_mean_absolute_error: 2.5577\n",
      "Epoch 40/100\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 7.0025 - mean_absolute_error: 1.7138 - val_loss: 14.9485 - val_mean_absolute_error: 2.8377\n",
      "Epoch 41/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 7.1814 - mean_absolute_error: 1.6818 - val_loss: 12.9646 - val_mean_absolute_error: 2.5369\n",
      "Epoch 42/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 6.7108 - mean_absolute_error: 1.6532 - val_loss: 14.9783 - val_mean_absolute_error: 2.7695\n",
      "Epoch 43/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 7.3206 - mean_absolute_error: 1.6746 - val_loss: 13.9985 - val_mean_absolute_error: 2.6252\n",
      "Epoch 44/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 6.8962 - mean_absolute_error: 1.6585 - val_loss: 14.4719 - val_mean_absolute_error: 2.7519\n",
      "Epoch 45/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 6.4899 - mean_absolute_error: 1.6664 - val_loss: 14.0614 - val_mean_absolute_error: 2.5918\n",
      "Epoch 46/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 6.3100 - mean_absolute_error: 1.6469 - val_loss: 12.9803 - val_mean_absolute_error: 2.5476\n",
      "Epoch 47/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 6.4552 - mean_absolute_error: 1.6203 - val_loss: 14.5579 - val_mean_absolute_error: 2.7100\n",
      "Epoch 48/100\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 6.5909 - mean_absolute_error: 1.6282 - val_loss: 12.8868 - val_mean_absolute_error: 2.5464\n",
      "Epoch 49/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 6.5351 - mean_absolute_error: 1.6397 - val_loss: 13.0469 - val_mean_absolute_error: 2.5005\n",
      "Epoch 50/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 6.1292 - mean_absolute_error: 1.5976 - val_loss: 17.1782 - val_mean_absolute_error: 3.0048\n",
      "Epoch 51/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 6.5329 - mean_absolute_error: 1.6737 - val_loss: 12.6096 - val_mean_absolute_error: 2.5183\n",
      "Epoch 52/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 6.5453 - mean_absolute_error: 1.6004 - val_loss: 12.8750 - val_mean_absolute_error: 2.4812\n",
      "Epoch 53/100\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 6.0352 - mean_absolute_error: 1.6201 - val_loss: 14.0510 - val_mean_absolute_error: 2.6644\n",
      "Epoch 54/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 6.1258 - mean_absolute_error: 1.5905 - val_loss: 13.9242 - val_mean_absolute_error: 2.6104\n",
      "Epoch 55/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 6.3631 - mean_absolute_error: 1.5610 - val_loss: 13.9465 - val_mean_absolute_error: 2.5925\n",
      "Epoch 56/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 5.9617 - mean_absolute_error: 1.5754 - val_loss: 13.0800 - val_mean_absolute_error: 2.4901\n",
      "Epoch 57/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 6.2934 - mean_absolute_error: 1.5354 - val_loss: 12.9129 - val_mean_absolute_error: 2.4297\n",
      "Epoch 58/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 5.8775 - mean_absolute_error: 1.5861 - val_loss: 13.6167 - val_mean_absolute_error: 2.6409\n",
      "Epoch 59/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 5.9685 - mean_absolute_error: 1.6056 - val_loss: 12.8798 - val_mean_absolute_error: 2.4785\n",
      "Epoch 60/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 6.0171 - mean_absolute_error: 1.6092 - val_loss: 13.2758 - val_mean_absolute_error: 2.5812\n",
      "Epoch 61/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 6.3836 - mean_absolute_error: 1.5575 - val_loss: 12.2851 - val_mean_absolute_error: 2.3854\n",
      "Epoch 62/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 5.8193 - mean_absolute_error: 1.4840 - val_loss: 14.4780 - val_mean_absolute_error: 2.6864\n",
      "Epoch 63/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 6.0106 - mean_absolute_error: 1.5245 - val_loss: 14.4660 - val_mean_absolute_error: 2.6742\n",
      "Epoch 64/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 5.4877 - mean_absolute_error: 1.5307 - val_loss: 13.7428 - val_mean_absolute_error: 2.6719\n",
      "Epoch 65/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 5.6047 - mean_absolute_error: 1.4994 - val_loss: 12.6383 - val_mean_absolute_error: 2.4764\n",
      "Epoch 66/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 5.7174 - mean_absolute_error: 1.5243 - val_loss: 14.6423 - val_mean_absolute_error: 2.7191\n",
      "Epoch 67/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 5.7568 - mean_absolute_error: 1.4973 - val_loss: 13.1553 - val_mean_absolute_error: 2.5138\n",
      "Epoch 68/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 5.6968 - mean_absolute_error: 1.5070 - val_loss: 13.4462 - val_mean_absolute_error: 2.6021\n",
      "Epoch 69/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 5.5717 - mean_absolute_error: 1.5441 - val_loss: 14.2021 - val_mean_absolute_error: 2.6652\n",
      "Epoch 70/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 5.6567 - mean_absolute_error: 1.4874 - val_loss: 13.3508 - val_mean_absolute_error: 2.5800\n",
      "Epoch 71/100\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 5.4375 - mean_absolute_error: 1.4420 - val_loss: 15.3119 - val_mean_absolute_error: 2.8300\n",
      "Epoch 72/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 5.2495 - mean_absolute_error: 1.5175 - val_loss: 14.3478 - val_mean_absolute_error: 2.7043\n",
      "Epoch 73/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 5.5086 - mean_absolute_error: 1.4959 - val_loss: 12.9965 - val_mean_absolute_error: 2.5210\n",
      "Epoch 74/100\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 5.3366 - mean_absolute_error: 1.4346 - val_loss: 13.2654 - val_mean_absolute_error: 2.5558\n",
      "Epoch 75/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 5.0192 - mean_absolute_error: 1.4283 - val_loss: 12.8985 - val_mean_absolute_error: 2.5718\n",
      "Epoch 76/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 5.1709 - mean_absolute_error: 1.4450 - val_loss: 12.8298 - val_mean_absolute_error: 2.5045\n",
      "Epoch 77/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 4.9800 - mean_absolute_error: 1.4496 - val_loss: 12.7101 - val_mean_absolute_error: 2.4955\n",
      "Epoch 78/100\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 5.1511 - mean_absolute_error: 1.4130 - val_loss: 13.0243 - val_mean_absolute_error: 2.5225\n",
      "Epoch 79/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 5.0040 - mean_absolute_error: 1.3962 - val_loss: 13.9200 - val_mean_absolute_error: 2.6491\n",
      "Epoch 80/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 5.1666 - mean_absolute_error: 1.4568 - val_loss: 13.4209 - val_mean_absolute_error: 2.5849\n",
      "Epoch 81/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 5.4566 - mean_absolute_error: 1.3732 - val_loss: 13.6537 - val_mean_absolute_error: 2.6582\n",
      "Epoch 82/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 5.1410 - mean_absolute_error: 1.4811 - val_loss: 13.7450 - val_mean_absolute_error: 2.5642\n",
      "Epoch 83/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 5.1647 - mean_absolute_error: 1.4516 - val_loss: 12.6312 - val_mean_absolute_error: 2.4712\n",
      "Epoch 84/100\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 5.0110 - mean_absolute_error: 1.4310 - val_loss: 13.9929 - val_mean_absolute_error: 2.6692\n",
      "Epoch 85/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 5.2449 - mean_absolute_error: 1.4513 - val_loss: 12.8510 - val_mean_absolute_error: 2.5269\n",
      "Epoch 86/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 4.8921 - mean_absolute_error: 1.4149 - val_loss: 12.9375 - val_mean_absolute_error: 2.5274\n",
      "Epoch 87/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 4.5319 - mean_absolute_error: 1.3717 - val_loss: 13.1106 - val_mean_absolute_error: 2.5407\n",
      "Epoch 88/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 4.6478 - mean_absolute_error: 1.3528 - val_loss: 14.2662 - val_mean_absolute_error: 2.6808\n",
      "Epoch 89/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 4.6304 - mean_absolute_error: 1.4215 - val_loss: 13.2423 - val_mean_absolute_error: 2.4927\n",
      "Epoch 90/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 4.6939 - mean_absolute_error: 1.3767 - val_loss: 14.3266 - val_mean_absolute_error: 2.7032\n",
      "Epoch 91/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 4.5441 - mean_absolute_error: 1.3275 - val_loss: 13.0897 - val_mean_absolute_error: 2.5974\n",
      "Epoch 92/100\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 4.7593 - mean_absolute_error: 1.3512 - val_loss: 13.1551 - val_mean_absolute_error: 2.4801\n",
      "Epoch 93/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 4.7722 - mean_absolute_error: 1.3457 - val_loss: 18.0189 - val_mean_absolute_error: 3.1116\n",
      "Epoch 94/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 4.7113 - mean_absolute_error: 1.4168 - val_loss: 12.2138 - val_mean_absolute_error: 2.3703\n",
      "Epoch 95/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 4.5505 - mean_absolute_error: 1.3151 - val_loss: 12.0723 - val_mean_absolute_error: 2.3857\n",
      "Epoch 96/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 4.5513 - mean_absolute_error: 1.2821 - val_loss: 13.1129 - val_mean_absolute_error: 2.5175\n",
      "Epoch 97/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 4.2864 - mean_absolute_error: 1.3762 - val_loss: 12.7609 - val_mean_absolute_error: 2.4741\n",
      "Epoch 98/100\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 4.3968 - mean_absolute_error: 1.3641 - val_loss: 14.0148 - val_mean_absolute_error: 2.6460\n",
      "Epoch 99/100\n",
      "303/303 [==============================] - 1s 4ms/step - loss: 4.2950 - mean_absolute_error: 1.3461 - val_loss: 12.5050 - val_mean_absolute_error: 2.4568\n",
      "Epoch 100/100\n",
      "303/303 [==============================] - 1s 4ms/step - loss: 4.6177 - mean_absolute_error: 1.3341 - val_loss: 13.0588 - val_mean_absolute_error: 2.4502\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "\n",
    "# define k-fold cross validation test\n",
    "kf = KFold(n_splits=4, random_state=seed)\n",
    "\n",
    "all_mae_histories = []\n",
    "\n",
    "for train, test in kf.split(train_data_scaled, train_targets):\n",
    "    # create model\n",
    "    model = build_model()\n",
    "    history = model.fit(train_data_scaled[train], \n",
    "              train_targets[train], \n",
    "              epochs=100, \n",
    "              batch_size=1, \n",
    "              verbose=1,\n",
    "              validation_data=(train_data_scaled[test], \n",
    "                               train_targets[test]))\n",
    "    mae_history = history.history['val_mean_absolute_error']\n",
    "    all_mae_histories.append(mae_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.06872889, 2.54058853, 2.73671882, 2.65800705])"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mae for each k-fold step\n",
    "np.mean(all_mae_histories,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The different runs do indeed show rather different validation scores, from 2.1 to 2.7. The average (2.5) is a much more reliable metric than any single score - that’s the entire point of K-fold cross-validation. In this case, you’re off by \\$2,500 on average, which is significant considering that the prices range from \\$10,000 to \\$50,000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.50101082025188"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(all_mae_histories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the history of successive mean K-fold validation scores\n",
    "num_epochs = 100\n",
    "average_mae_history = [np.mean([x[i] for x in all_mae_histories]) for i in range(num_epochs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x18284feb8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(range(1, len(average_mae_history) + 1), average_mae_history)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Validation MAE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum MAE: 2.2985\n",
      "Epoch: 69\n"
     ]
    }
   ],
   "source": [
    "epochs_min = np.argmin(average_mae_history)\n",
    "print('Minimum MAE: {:.4f}\\nEpoch: {:d}'.format(average_mae_history[epochs_min],epochs_min))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102/102 [==============================] - 0s 4ms/step\n"
     ]
    }
   ],
   "source": [
    "# Training the final model\n",
    "\n",
    "model = build_model()\n",
    "model.fit(train_data_scaled, \n",
    "          train_targets,\n",
    "          epochs=69, \n",
    "          batch_size=1, \n",
    "          verbose=0)\n",
    "\n",
    "test_mse_score, test_mae_score = model.evaluate(test_data_scaled, test_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.3270663373610554"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_mae_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Wrapping up\n",
    "\n",
    "Here’s what you should take away from this example:\n",
    "\n",
    "- **Mean squared error (MSE)** is a **loss function** commonly used for **regression**.\n",
    "- A common **regression metric** is **mean absolute error (MAE)**.\n",
    "- When **features** in the input data have values in **different ranges**, each feature should be **scaled independently** as a preprocessing step.\n",
    "- When there is **little data available, using K-fold validation** is a great way to reliably evaluate a model.\n",
    "- When **little training data** is available, it’s preferable to use a **small network with few hidden layers** (typically only one or two), in order to avoid severe overfitting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Chapter summary\n",
    "\n",
    "- You’re now able to handle the most common kinds of machine-learning tasks on vector data: **binary classification, multiclass classification, and scalar regression**. \n",
    "- You’ll usually need to **preprocess raw data** before feeding it into a neural network.\n",
    "- When your data has features with different ranges, **scale each feature** independently as part of preprocessing.\n",
    "- As training progresses, neural networks eventually begin to **overfit** and obtain worse results on never-before-seen data.\n",
    "- **If you don’t have much training data, use a small network** with only one or two hidden layers, to avoid severe overfitting.\n",
    "- If your data is divided into **many categories**, you may cause information **bottlenecks if you make the intermediate layers too small**.\n",
    "- Regression uses different loss functions and different evaluation metrics than classification.\n",
    "- When you’re working with **little data, K-fold** validation can help reliably evaluate your model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
